{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BaIK-bqzFhja"
   },
   "source": [
    "## Generate caption for valid set\n",
    "\n",
    "This notebook is built based on our baseline model pytorch v3 version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m2p8iXQygqRN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from time import time\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import chain\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import models, transforms, datasets\n",
    "from torchsummary import summary\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "START = \"startseq\"\n",
    "STOP = \"endseq\"\n",
    "EPOCHS = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VlBS8URRgaOo"
   },
   "source": [
    "## 1. Load dataset\n",
    "\n",
    "- Read captions\n",
    "- Preprocess captions\n",
    "\n",
    "> We need image paths, word lookup tables, and max_length in generating function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_captioning = \"../../s3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ra-d7BLlf1nC"
   },
   "outputs": [],
   "source": [
    "def get_img_info(name, num=np.inf):\n",
    "    \"\"\"\n",
    "    Returns img paths and captions\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    name: str\n",
    "        the json file name\n",
    "    num: int (default: np.inf)\n",
    "        the number of observations to get\n",
    "\n",
    "    Return:\n",
    "    --------\n",
    "    list, dict, int\n",
    "        img paths, corresponding captions, max length of captions\n",
    "    \"\"\"\n",
    "    img_path = []\n",
    "    caption = [] \n",
    "    max_length = 0\n",
    "    with open(f'{root_captioning}/json/{name}.json', 'r') as json_data:\n",
    "        data = json.load(json_data)\n",
    "        for filename in data.keys():\n",
    "            if num is not None and len(caption) == num:\n",
    "                break\n",
    "            img_path.append(\n",
    "                f'{root_captioning}/{name}/{filename}'\n",
    "            )\n",
    "            sen_list = []\n",
    "            for sentence in data[filename]['sentences']:\n",
    "                max_length = max(max_length, len(sentence['tokens']))\n",
    "                sen_list.append(sentence['raw'])\n",
    "\n",
    "            caption.append(sen_list)\n",
    "    \n",
    "    return img_path, caption, max_length            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zMUxfR9xKICI"
   },
   "outputs": [],
   "source": [
    "# get img path and caption list\n",
    "# only test 800 train samples and 200 valid samples\n",
    "train_paths, train_descriptions, max_length_train = get_img_info('train')\n",
    "test_paths, test_descriptions, max_length_test = get_img_info('valid')\n",
    "max_length = max(max_length_train, max_length_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "Gnq3ZjaSU79-",
    "outputId": "f6b8c744-6a8b-4694-a701-0061b170925a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8332\n",
      "2084\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "print(len(train_descriptions)) # How many images? \n",
    "print(len(test_descriptions)) # How many images? \n",
    "print(max_length) # Maximum length of a caption (in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ZZQZkT8X_qNt",
    "outputId": "bef950fb-de4d-487c-b033-06a3fddc0ff0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8332\n",
      "2084\n"
     ]
    }
   ],
   "source": [
    "print(len(train_paths))\n",
    "print(len(test_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2ZQ_Cl71YM55",
    "outputId": "25a3a493-16f3-42b3-fe8b-a11908288b0b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../s3/train/ucm_1080.jpg'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cBX6TJbPF0nD"
   },
   "outputs": [],
   "source": [
    "# add start and stop token\n",
    "for v in train_descriptions: \n",
    "  for d in range(len(v)):\n",
    "    v[d] = f'{START} {v[d]} {STOP}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "v3_PCjddHcUe",
    "outputId": "fa72020b-9013-4961-fc4a-190f74b2db86"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['startseq Lots of boats docked at the harbor and the boats are closed to each other . endseq',\n",
       " 'startseq Lots of boats docked neatly at the harbor . endseq',\n",
       " 'startseq Many boats docked neatly at the harbor and the water is deep blue . endseq',\n",
       " 'startseq Many boats docked neatly at the harbor and some positions are free . endseq',\n",
       " 'startseq Lots of boats docked neatly at the harbor and the boats are closed to each other . endseq']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_descriptions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "C8DFzzPmXg-n",
    "outputId": "5511898b-1f3e-4463-aac7-ec698b94e940"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41660"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# append all train captions\n",
    "all_train_captions = []\n",
    "for val in train_descriptions:\n",
    "    for cap in val:\n",
    "        all_train_captions.append(cap)\n",
    "len(all_train_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "w2vQHy7Ja2Py",
    "outputId": "9df3119a-b871-49c9-c27f-fe248663db9a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'startseq Lots of boats docked at the harbor and the boats are closed to each other . endseq'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_captions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-HxHSgLgYUUE",
    "outputId": "d07d4071-7df2-435c-893f-917bc9dba504"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed words 2704 ==> 901\n"
     ]
    }
   ],
   "source": [
    "# remove words occur less than 10 times\n",
    "word_count_threshold = 10\n",
    "word_counts = {}\n",
    "nsents = 0\n",
    "for sent in all_train_captions:\n",
    "    nsents += 1\n",
    "    for w in sent.split(' '):\n",
    "        word_counts[w] = word_counts.get(w, 0) + 1\n",
    "\n",
    "vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
    "print('preprocessed words %d ==> %d' % (len(word_counts), len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Oi3zqAUhYcNP",
    "outputId": "b104cf97-4c3a-4b01-a5c4-0022ffac7fa1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "902"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build lookup tables\n",
    "idxtoword = {}\n",
    "wordtoidx = {}\n",
    "\n",
    "ix = 1\n",
    "for w in vocab:\n",
    "    wordtoidx[w] = ix\n",
    "    idxtoword[ix] = w\n",
    "    ix += 1\n",
    "    \n",
    "vocab_size = len(idxtoword) + 1 \n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "KSohY53bYjTi",
    "outputId": "1fc5592c-13af-402d-d8e4-8d70fbd1f410"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "# adjust max_length\n",
    "max_length +=2\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "w33HKER-vZ7U",
    "outputId": "7eb9a2c0-1811-4141-c278-252449351c85"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# load device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ygXvdSURHYUz"
   },
   "source": [
    "## 2. Load functions (they are required for loading trained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DWOP58lrtGwt"
   },
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained=True):\n",
    "        \"\"\"\n",
    "        Initializes a CNNModel\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        pretrained: bool (default: True)\n",
    "            use pretrained model if True\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        super(CNNModel, self).__init__()\n",
    "      \n",
    "        # remove the classification layer\n",
    "        layers = list(model.children())\n",
    "        self.Conv2d_1a_3x3 = layers[0]\n",
    "        self.Conv2d_2a_3x3 = layers[1]\n",
    "        self.Conv2d_2b_3x3 = layers[2]\n",
    "        self.Conv2d_3b_1x1 = layers[3]\n",
    "        self.Conv2d_4a_3x3 = layers[4]\n",
    "        self.Mixed_5b = layers[5]\n",
    "        self.Mixed_5c = layers[6]\n",
    "        self.Mixed_5d = layers[7]\n",
    "        self.Mixed_6a = layers[8]\n",
    "        self.Mixed_6b = layers[9]\n",
    "        self.Mixed_6c = layers[10]\n",
    "        self.Mixed_6d = layers[11]\n",
    "        self.Mixed_6e = layers[12]\n",
    "        self.Mixed_7a = layers[13]\n",
    "        self.Mixed_7b = layers[14]\n",
    "        self.Mixed_7c = layers[15]\n",
    "\n",
    "        self.input_size = 299\n",
    "\n",
    "    def forward(self, x, train=False):\n",
    "        \"\"\"\n",
    "        forward of the CNNModel\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        img_input: torch.Tensor\n",
    "            the image matrix\n",
    "        train: bool (default: False)\n",
    "            use the model only for feature extraction if False\n",
    "\n",
    "        Return:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            image feature matrix\n",
    "        \"\"\"\n",
    "#         if not train:\n",
    "#           # set the model to evaluation model\n",
    "#           self.model.eval()\n",
    "\n",
    "        # N x 3 x 299 x 299\n",
    "        x = self.Conv2d_1a_3x3(x)\n",
    "        # N x 32 x 149 x 149\n",
    "        x = self.Conv2d_2a_3x3(x)\n",
    "        # N x 32 x 147 x 147\n",
    "        x = self.Conv2d_2b_3x3(x)\n",
    "        # N x 64 x 147 x 147\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        # N x 64 x 73 x 73\n",
    "        x = self.Conv2d_3b_1x1(x)\n",
    "        # N x 80 x 73 x 73\n",
    "        x = self.Conv2d_4a_3x3(x)\n",
    "        # N x 192 x 71 x 71\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        # N x 192 x 35 x 35\n",
    "        x = self.Mixed_5b(x)\n",
    "        # N x 256 x 35 x 35\n",
    "        x = self.Mixed_5c(x)\n",
    "        # N x 288 x 35 x 35\n",
    "        x = self.Mixed_5d(x)\n",
    "        # N x 288 x 35 x 35\n",
    "        x = self.Mixed_6a(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_6b(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_6c(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_6d(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_6e(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_7a(x)\n",
    "        # N x 1280 x 8 x 8\n",
    "        x = self.Mixed_7b(x)\n",
    "        # N x 2048 x 8 x 8\n",
    "        x = self.Mixed_7c(x)\n",
    "        # N x 2048 x 8 x 8\n",
    "        # Adaptive average pooling\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        # N x 2048 x 1 x 1\n",
    "        x = F.dropout(x, training=train)\n",
    "        # N x 2048 x 1 x 1\n",
    "        x = torch.flatten(x, 1)\n",
    "        # N x 2048\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5uWZ8QOOXtUM"
   },
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size, \n",
    "        embedding_dim, \n",
    "        hidden_size=256,\n",
    "        embedding_matrix=None, \n",
    "        embedding_train=False\n",
    "    ):\n",
    "      \n",
    "        \"\"\"\n",
    "        Initializes a RNNModel\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        vocab_size: int\n",
    "            the size of the vocabulary\n",
    "        embedding_dim: int\n",
    "            the number of features in the embedding matrix\n",
    "        hidden_size: int (default: 256)\n",
    "            the size of the hidden state in LSTM\n",
    "        embedding_matrix: torch.Tensor (default: None)\n",
    "            if not None, use this matrix as the embedding matrix\n",
    "        embedding_train: bool (default: False)\n",
    "            not train the embedding matrix if False\n",
    "        \"\"\"\n",
    "\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "\n",
    "        if embedding_matrix is not None:\n",
    "\n",
    "            self.embedding.load_state_dict({\n",
    "              'weight': torch.FloatTensor(embedding_matrix)\n",
    "            })\n",
    "            self.embedding.requires_grad = embedding_train\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
    " \n",
    "\n",
    "    def forward(self, captions):\n",
    "        \"\"\"\n",
    "        forward of the RNNModel\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        captions: torch.Tensor\n",
    "            the padded caption matrix\n",
    "\n",
    "        Return:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            word probabilities for each position\n",
    "        \"\"\"\n",
    "\n",
    "        # embed the captions\n",
    "        embedding = self.dropout(self.embedding(captions))\n",
    "\n",
    "        outputs, (h, c) = self.lstm(embedding)\n",
    "\n",
    "        return outputs, (h, c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vzigDNU6jEly"
   },
   "outputs": [],
   "source": [
    "class CaptionModel(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        cnn_type, \n",
    "        vocab_size, \n",
    "        embedding_dim, \n",
    "        hidden_size=256,\n",
    "        embedding_matrix=None, \n",
    "        embedding_train=False\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        Initializes a CaptionModel\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        cnn_type: str\n",
    "            the CNN type, either 'vgg16' or 'inception_v3'\n",
    "        vocab_size: int\n",
    "            the size of the vocabulary\n",
    "        embedding_dim: int\n",
    "            the number of features in the embedding matrix\n",
    "        feature_size: int\n",
    "            the number of features in the image matrix\n",
    "        hidden_size: int (default: 256)\n",
    "            the size of the hidden state in LSTM\n",
    "        embedding_matrix: torch.Tensor (default: None)\n",
    "            if not None, use this matrix as the embedding matrix\n",
    "        embedding_train: bool (default: False)\n",
    "            not train the embedding matrix if False\n",
    "        \"\"\"    \n",
    "        super(CaptionModel, self).__init__() \n",
    "\n",
    "        # set feature_size based on cnn_type\n",
    "        if cnn_type == 'vgg16':\n",
    "            self.feature_size = 4096\n",
    "        elif cnn_type == 'inception_v3':\n",
    "            self.feature_size = 2048\n",
    "        else:\n",
    "            raise Exception(\"Please choose between 'vgg16' and 'inception_v3'.\")  \n",
    "\n",
    "        self.decoder = RNNModel(\n",
    "            vocab_size, \n",
    "            embedding_dim,\n",
    "            hidden_size,\n",
    "            embedding_matrix,\n",
    "            embedding_train\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.dense1 = nn.Linear(self.feature_size, hidden_size) \n",
    "        self.relu1 = nn.ReLU()\n",
    "          \n",
    "        self.dense2 = nn.Linear(hidden_size, hidden_size) \n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dense3 = nn.Linear(hidden_size, vocab_size) \n",
    "\n",
    "    def forward(self, img_features, captions):\n",
    "        \"\"\"\n",
    "        forward of the CaptionModel\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        img_features: torch.Tensor\n",
    "            the image feature matrix\n",
    "        captions: torch.Tensor\n",
    "            the padded caption matrix\n",
    "\n",
    "        Return:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            word probabilities for each position\n",
    "        \"\"\"\n",
    "\n",
    "        img_features =\\\n",
    "        self.relu1(\n",
    "            self.dense1(\n",
    "                self.dropout(\n",
    "                    img_features\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        decoder_out, _ = self.decoder(captions)\n",
    "\n",
    "        # add up decoder outputs and image features\n",
    "        outputs =\\\n",
    "        self.dense3(\n",
    "            self.relu2(\n",
    "                self.dense2(\n",
    "                    decoder_out.add(\n",
    "                        (img_features.view(img_features.size(0), 1, -1))\\\n",
    "                        .repeat(1, decoder_out.size(1), 1)\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wCSzCGtB8-KM"
   },
   "source": [
    "## 3. Load pre-extracted image features and pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dyCHBbjnf2b1"
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "model_path = f'{root_captioning}/training_outputs/caption-model_pytorch_inc_v3_full.hdf5'\n",
    "caption_model = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hcP-9HrlD60n"
   },
   "outputs": [],
   "source": [
    "# load img features\n",
    "with open(f'{root_captioning}/training_outputs/test_pytorch_inc_v3_full.pkl', 'rb') as f:\n",
    "    test_img_features = pickle.load(f)\n",
    "\n",
    "with open(f'{root_captioning}/training_outputs/train_pytorch_inc_v3_full.pkl', 'rb') as f2:\n",
    "    train_img_features = pickle.load(f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ymx07aUHpHMZ"
   },
   "source": [
    "## 4. Generate captions on test data and store as json data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a5WoWWbc9Y-g"
   },
   "outputs": [],
   "source": [
    "# load generating function\n",
    "def generateCaption(img_features):\n",
    "    in_text = START\n",
    "\n",
    "    for i in range(max_length):\n",
    "\n",
    "        sequence = [wordtoidx[w] for w in in_text.split() if w in wordtoidx]\n",
    "        sequence = np.pad(sequence, (0, max_length - len(sequence)),\n",
    "                          mode='constant', constant_values=(0, 0))\n",
    "        caption_model.eval()\n",
    "        yhat = caption_model(\n",
    "            torch.FloatTensor(img_features)\\\n",
    "            .view(-1, caption_model.feature_size).to(device),\n",
    "            torch.LongTensor(sequence).view(-1, max_length).to(device)\n",
    "        )\n",
    "\n",
    "        yhat = yhat.view(-1, vocab_size).argmax(1)\n",
    "        word = idxtoword[yhat.cpu().data.numpy()[i]]\n",
    "        in_text += ' ' + word\n",
    "        if word == STOP:\n",
    "            break\n",
    "    final = in_text.split()\n",
    "    final = final[1 : -1]\n",
    "    final = ' '.join(final)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate results\n",
    "results = {}\n",
    "for n in range(len(test_paths)):\n",
    "    pic = test_paths[n]\n",
    "    # note the filename splitting depends on path\n",
    "    filename = pic.split('/')[4]\n",
    "    img_features = test_img_features[n]\n",
    "    generated = generateCaption(img_features)\n",
    "    results[filename] = generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rsicd_park_3.jpg': 'many green trees and some buildings are in a park .',\n",
       " 'rsicd_mountain_176.jpg': 'it is a piece of irregular khaki mountain .',\n",
       " 'rsicd_denseresidential_210.jpg': 'many buildings and green trees are in a dense residential area .',\n",
       " 'rsicd_school_163.jpg': 'a baseball field is surrounded by some green trees and several buildings .',\n",
       " 'ucm_997.jpg': 'A house with verdant lawn surrounded and a road beside in the sparse residential area .',\n",
       " 'rsicd_industrial_240.jpg': 'many buildings are in an industrial area .',\n",
       " 'rsicd_00613.jpg': 'a playground is surrounded by many buildings and some green trees .',\n",
       " 'ucm_637.jpg': 'An intersection with two roads vertical to each other .',\n",
       " 'rsicd_commercial_190.jpg': 'many buildings are in a commercial area .',\n",
       " 'rsicd_meadow_94.jpg': 'it is a piece of khaki bare land.'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview few of results\n",
    "dict(list(results.items())[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results in github repo\n",
    "save_path = '../../591_capstone_2020-mda-mds/models/'\n",
    "with open(save_path + 'test_results.json', 'w') as fp:\n",
    "    json.dump(results, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results in s3 bucket\n",
    "with open( f'{root_captioning}/training_outputs/test_results.json', 'w') as fp:\n",
    "    json.dump(results, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "10-dq_evaluation_metrics.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
