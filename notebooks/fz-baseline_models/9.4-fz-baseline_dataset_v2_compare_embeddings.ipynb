{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m2p8iXQygqRN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from time import time\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import models, transforms, datasets\n",
    "from torchsummary import summary\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from nltk.translate import bleu_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "START = \"startseq\"\n",
    "STOP = \"endseq\"\n",
    "EPOCHS = 10\n",
    "AWS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "np.random.seed(123)\n",
    "# torch.manual_seed(23964)\n",
    "# np.random.seed(6457)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vmle0AXFBSdk"
   },
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "# import gc \n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kEMwH956gaOG"
   },
   "source": [
    "The following function is used to nicely format elapsed times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mGt4swcqIFKR"
   },
   "outputs": [],
   "source": [
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return f\"{h}:{m:>02}:{s:>05.2f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_7jgA6i88Iv8"
   },
   "source": [
    "### Google CoLab or AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "colab_type": "code",
    "id": "2W5oWBSofR7d",
    "outputId": "a0cc11ef-0e70-470c-e4b5-53912f21e341"
   },
   "outputs": [],
   "source": [
    "if AWS:\n",
    "    root_captioning = \"../../data\"\n",
    "else:\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive', force_remount=True)\n",
    "        root_captioning = \"/content/drive/My Drive/data\"\n",
    "        COLAB = True\n",
    "        print(\"Note: using Google CoLab\")\n",
    "    except:\n",
    "        print(\"Note: not using Google CoLab\")\n",
    "        COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VlBS8URRgaOo"
   },
   "source": [
    "### Clean/Build Dataset\n",
    "\n",
    "- Read captions\n",
    "- Preprocess captions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ra-d7BLlf1nC"
   },
   "outputs": [],
   "source": [
    "def get_img_info(name, num=np.inf):\n",
    "    \"\"\"\n",
    "    Returns img paths and captions\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    name: str\n",
    "        the json file name\n",
    "    num: int (default: np.inf)\n",
    "        the number of observations to get\n",
    "\n",
    "    Return:\n",
    "    --------\n",
    "    list, dict, int\n",
    "        img paths, corresponding captions, max length of captions\n",
    "    \"\"\"\n",
    "    img_path = []\n",
    "    caption = [] \n",
    "    max_length = 0\n",
    "    if AWS:\n",
    "        with open(f'{root_captioning}/json/{name}.json', 'r') as json_data:\n",
    "            data = json.load(json_data)\n",
    "            for filename in data.keys():\n",
    "                if num is not None and len(caption) == num:\n",
    "                    break\n",
    "                img_path.append(\n",
    "                    f'{root_captioning}/{name}/{filename}'\n",
    "                )\n",
    "                sen_list = []\n",
    "                for sentence in data[filename]['sentences']:\n",
    "                    max_length = max(max_length, len(sentence['tokens']))\n",
    "                    sen_list.append(sentence['raw'])\n",
    "\n",
    "                caption.append(sen_list)    \n",
    "    else:            \n",
    "        with open(f'{root_captioning}/interim/{name}.json', 'r') as json_data:\n",
    "            data = json.load(json_data)\n",
    "            for set_name in ['rsicd', 'ucm']:\n",
    "                for filename in data[set_name].keys():\n",
    "                    if num is not None and len(caption) == num:\n",
    "                        break\n",
    "\n",
    "                    img_path.append(\n",
    "                        f'{root_captioning}/raw/imgs/{set_name}/{filename}'\n",
    "                    )\n",
    "                    sen_list = []\n",
    "                    for sentence in data[set_name][filename]['sentences']:\n",
    "                        max_length = max(max_length, len(sentence['tokens']))\n",
    "                        sen_list.append(sentence['raw'])\n",
    "\n",
    "                    caption.append(sen_list)\n",
    "    \n",
    "    return img_path, caption, max_length            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zMUxfR9xKICI"
   },
   "outputs": [],
   "source": [
    "# get img path and caption list\n",
    "# only test 800 train samples and 200 valid samples\n",
    "# train_paths, train_descriptions, max_length_train = get_img_info('train', 800)\n",
    "# test_paths, test_descriptions, max_length_test = get_img_info('valid', 200)\n",
    "\n",
    "train_paths, train_descriptions, max_length_train = get_img_info('train')\n",
    "test_paths, test_descriptions, max_length_test = get_img_info('valid')\n",
    "max_length = max(max_length_train, max_length_test)\n",
    "\n",
    "      \n",
    "lex = set()\n",
    "for sen in train_descriptions:\n",
    "    [lex.update(d.split()) for d in sen]\n",
    "\n",
    "for sen in test_descriptions:\n",
    "    [lex.update(d.split()) for d in sen]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xdFUtyIPgaOy"
   },
   "source": [
    "Stats on what was collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "Gnq3ZjaSU79-",
    "outputId": "8e97d73f-d09e-41ae-eaef-30f3affe9ec7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8332\n",
      "2084\n",
      "2912\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "print(len(train_descriptions)) # How many images? \n",
    "print(len(test_descriptions)) # How many images? \n",
    "print(len(lex)) # How many unique words (vocab)\n",
    "print(max_length) # Maximum length of a caption (in words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aP2-k6XfgaPT"
   },
   "source": [
    "Display the size of the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ZZQZkT8X_qNt",
    "outputId": "be83f657-a127-4f5a-a078-f766e47e2299"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8332\n",
      "2084\n"
     ]
    }
   ],
   "source": [
    "print(len(train_paths))\n",
    "print(len(test_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2ZQ_Cl71YM55",
    "outputId": "3cacbb52-3c08-450b-90ab-35105b79121c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../data/train/ucm_1080.jpg'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_paths[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iawCdtU4gaPa"
   },
   "source": [
    "Build the sequences.  We include a **start** and **stop** token at the beginning/end.  We will later use the **start** token to begin the process of generating a caption.  Encountering the **stop** token in the generated text will let us know we are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cBX6TJbPF0nD"
   },
   "outputs": [],
   "source": [
    "for v in train_descriptions: \n",
    "    for d in range(len(v)):\n",
    "        v[d] = f'{START} {v[d]} {STOP}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XHD6IZ3HgaPk"
   },
   "source": [
    "See how many discriptions were extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "colab_type": "code",
    "id": "v3_PCjddHcUe",
    "outputId": "2cb28d45-8591-4b37-c14c-083d328f0ab3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['startseq Lots of boats docked at the harbor and the boats are closed to each other . endseq',\n",
       " 'startseq Lots of boats docked neatly at the harbor . endseq',\n",
       " 'startseq Many boats docked neatly at the harbor and the water is deep blue . endseq',\n",
       " 'startseq Many boats docked neatly at the harbor and some positions are free . endseq',\n",
       " 'startseq Lots of boats docked neatly at the harbor and the boats are closed to each other . endseq']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_descriptions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "C8DFzzPmXg-n",
    "outputId": "ef8ed03b-6b24-4e7d-c320-da83e8dd3905"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41660"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_captions = []\n",
    "for val in train_descriptions:\n",
    "    for cap in val:\n",
    "        all_train_captions.append(cap)\n",
    "len(all_train_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "w2vQHy7Ja2Py",
    "outputId": "69054dd7-fb73-4ed9-b543-c051410a21fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'startseq Lots of boats docked at the harbor and the boats are closed to each other . endseq'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_captions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KiTjOC_9x1_x"
   },
   "source": [
    "Words that do not occur very often can be misleading to neural network training.  It is better to simply remove such words.  Here we remove any words that occur less than 10 times.  We display what the total vocabulary shrunk to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-HxHSgLgYUUE",
    "outputId": "76262f7c-ee9f-4c4d-fc7b-1836abe0014f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed words 2704 ==> 901\n"
     ]
    }
   ],
   "source": [
    "word_count_threshold = 10\n",
    "word_counts = {}\n",
    "nsents = 0\n",
    "for sent in all_train_captions:\n",
    "    nsents += 1\n",
    "    for w in sent.split(' '):\n",
    "        word_counts[w] = word_counts.get(w, 0) + 1\n",
    "\n",
    "vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
    "print('preprocessed words %d ==> %d' % (len(word_counts), len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E3vQYH860N8J"
   },
   "source": [
    "Next we build two lookup tables for this vocabulary. One idxtoword convers index numbers to actual words to index values.  The wordtoidx lookup table performs the opposit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Oi3zqAUhYcNP",
    "outputId": "7219e969-1627-4f45-d99f-6fad98ddd577"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "902"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxtoword = {}\n",
    "wordtoidx = {}\n",
    "\n",
    "ix = 1\n",
    "for w in vocab:\n",
    "    wordtoidx[w] = ix\n",
    "    idxtoword[ix] = w\n",
    "    ix += 1\n",
    "    \n",
    "vocab_size = len(idxtoword) + 1 \n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rxs9sfOq0Xxz"
   },
   "source": [
    "Previously we added a start and stop token to all sentences.  We must account for this in the maximum length of captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "KSohY53bYjTi",
    "outputId": "f23f5729-3844-4a5f-a5ef-ce517bb07f15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "max_length +=2\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vtIIFt4m6pTv"
   },
   "source": [
    "### Loading Wikipedia2vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read the embedding matrix \n",
    "# with open(f'{root_captioning}/enwiki_20180420_2338_words_500d.json', 'r', encoding='utf-8') as file:\n",
    "#     embeddings_index = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_dim = 500\n",
    "\n",
    "# # Get 200-dim dense vector for each of the 10000 words in out vocabulary\n",
    "# embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "# count =0\n",
    "# for word, i in wordtoidx.items():\n",
    "#     #if i < max_words:\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         count += 1\n",
    "#         # Words not found in the embedding index will be all zeros\n",
    "#         embedding_matrix[i] = np.array(embedding_vector)\n",
    "\n",
    "# print(f'{count} out of {vocab_size} words are found in the pre-trained matrix.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vtIIFt4m6pTv"
   },
   "source": [
    "### Loading Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "IY_9XZ4Hec73",
    "outputId": "326edc88-1620-4698-a90b-5f168b215185"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:22, 18129.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {} \n",
    "path = os.path.join(root_captioning, 'glove.6B.200d.txt') if AWS\\\n",
    "else os.path.join(root_captioning, 'raw', 'glove.6B.200d.txt')\n",
    "\n",
    "f = open(\n",
    "    path, \n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "\n",
    "f.close()\n",
    "print(f'Found {len(embeddings_index)} word vectors.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "adEmnYFEfog6"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 200\n",
    "\n",
    "# Get 200-dim dense vector for each of the 10000 words in out vocabulary\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "for word, i in wordtoidx.items():\n",
    "    #if i < max_words:\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in the embedding index will be all zeros\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FKynFo2xJiOD",
    "outputId": "e4bdfe93-1bf8-4c2d-b164-3eec0986d391"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(902, 200)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kYH3-S2n82MN"
   },
   "source": [
    "### Building the Neural Network\n",
    "\n",
    "An embedding matrix is built from Glove.  This will be directly copied to the weight matrix of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "w33HKER-vZ7U",
    "outputId": "c4b885b3-55a9-4b68-bc99-b176f676a65d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DWOP58lrtGwt"
   },
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained=True):\n",
    "        \"\"\"\n",
    "        Initializes a CNNModel\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        pretrained: bool (default: True)\n",
    "            use pretrained model if True\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        super(CNNModel, self).__init__()\n",
    "\n",
    "        # inception v3 expects (299, 299) sized images\n",
    "        self.model = models.inception_v3(pretrained=pretrained, aux_logits=False)\n",
    "        # remove the classification layer\n",
    "        self.model =\\\n",
    "        nn.Sequential(\n",
    "            *(list(self.model.children())[: 3]),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            *(list(self.model.children())[3: 5]),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            *(list(self.model.children())[5: -1])\n",
    "        )\n",
    "\n",
    "        self.input_size = 299\n",
    "\n",
    "    def forward(self, img_input, train=False):\n",
    "        \"\"\"\n",
    "        forward of the CNNModel\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        img_input: torch.Tensor\n",
    "            the image matrix\n",
    "        train: bool (default: False)\n",
    "            use the model only for feature extraction if False\n",
    "\n",
    "        Return:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            image feature matrix\n",
    "        \"\"\"\n",
    "        if not train:\n",
    "          # set the model to evaluation model\n",
    "          self.model.eval()\n",
    "\n",
    "        # N x 3 x 299 x 299\n",
    "        features = self.model(img_input)\n",
    "        # N x 2048 x 8 x 8\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5uWZ8QOOXtUM"
   },
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size, \n",
    "        embedding_dim, \n",
    "        hidden_size=256,\n",
    "        embedding_matrix=None, \n",
    "        embedding_train=False\n",
    "    ):\n",
    "      \n",
    "        \"\"\"\n",
    "        Initializes a RNNModel\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        vocab_size: int\n",
    "            the size of the vocabulary\n",
    "        embedding_dim: int\n",
    "            the number of features in the embedding matrix\n",
    "        hidden_size: int (default: 256)\n",
    "            the size of the hidden state in LSTM\n",
    "        embedding_matrix: torch.Tensor (default: None)\n",
    "            if not None, use this matrix as the embedding matrix\n",
    "        embedding_train: bool (default: False)\n",
    "            not train the embedding matrix if False\n",
    "        \"\"\"\n",
    "\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "\n",
    "        if embedding_matrix is not None:\n",
    "\n",
    "            self.embedding.load_state_dict({\n",
    "                'weight': torch.FloatTensor(embedding_matrix)\n",
    "            })\n",
    "            self.embedding.weight.requires_grad = embedding_train\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
    " \n",
    "\n",
    "    def forward(self, captions):\n",
    "        \"\"\"\n",
    "        forward of the RNNModel\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        captions: torch.Tensor\n",
    "            the padded caption matrix\n",
    "\n",
    "        Return:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            word probabilities for each position\n",
    "        \"\"\"\n",
    "\n",
    "        # embed the captions\n",
    "        embedding = self.dropout(self.embedding(captions))\n",
    "\n",
    "        outputs, (h, c) = self.lstm(embedding)\n",
    "\n",
    "        return outputs, (h, c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vzigDNU6jEly"
   },
   "outputs": [],
   "source": [
    "class CaptionModel(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        cnn_type, \n",
    "        vocab_size, \n",
    "        embedding_dim, \n",
    "        hidden_size=256,\n",
    "        embedding_matrix=None, \n",
    "        embedding_train=False\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        Initializes a CaptionModel\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        cnn_type: str\n",
    "            the CNN type, either 'vgg16' or 'inception_v3'\n",
    "        vocab_size: int\n",
    "            the size of the vocabulary\n",
    "        embedding_dim: int\n",
    "            the number of features in the embedding matrix\n",
    "        feature_size: int\n",
    "            the number of features in the image matrix\n",
    "        hidden_size: int (default: 256)\n",
    "            the size of the hidden state in LSTM\n",
    "        embedding_matrix: torch.Tensor (default: None)\n",
    "            if not None, use this matrix as the embedding matrix\n",
    "        embedding_train: bool (default: False)\n",
    "            not train the embedding matrix if False\n",
    "        \"\"\"    \n",
    "        super(CaptionModel, self).__init__() \n",
    "\n",
    "        # set feature_size based on cnn_type\n",
    "        if cnn_type == 'vgg16':\n",
    "            self.feature_size = 4096\n",
    "        elif cnn_type == 'inception_v3':\n",
    "            self.feature_size = 2048\n",
    "        else:\n",
    "            raise Exception(\"Please choose between 'vgg16' and 'inception_v3'.\")  \n",
    "\n",
    "        self.decoder = RNNModel(\n",
    "            vocab_size, \n",
    "            embedding_dim,\n",
    "            hidden_size,\n",
    "            embedding_matrix,\n",
    "            embedding_train\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.dense1 = nn.Linear(self.feature_size, hidden_size) \n",
    "        self.relu1 = nn.ReLU()\n",
    "          \n",
    "        self.dense2 = nn.Linear(hidden_size, hidden_size) \n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dense3 = nn.Linear(hidden_size, vocab_size) \n",
    "\n",
    "    def forward(self, img_features, captions):\n",
    "        \"\"\"\n",
    "        forward of the CaptionModel\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        img_features: torch.Tensor\n",
    "            the image feature matrix\n",
    "        captions: torch.Tensor\n",
    "            the padded caption matrix\n",
    "\n",
    "        Return:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            word probabilities for each position\n",
    "        \"\"\"\n",
    "\n",
    "        img_features =\\\n",
    "        self.relu1(\n",
    "            self.dense1(\n",
    "                self.dropout(\n",
    "                    img_features\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        decoder_out, _ = self.decoder(captions)\n",
    "\n",
    "        # add up decoder outputs and image features\n",
    "        outputs =\\\n",
    "        self.dense3(\n",
    "            self.relu2(\n",
    "                self.dense2(\n",
    "                    decoder_out.add(\n",
    "                        (img_features.view(img_features.size(0), 1, -1))\\\n",
    "                        .repeat(1, decoder_out.size(1), 1)\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wCSzCGtB8-KM"
   },
   "source": [
    "### Train the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R0ihg_mBrF-b"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \"\"\"\n",
    "    train the CaptionModel\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    model: CaptionModel\n",
    "        a CaptionModel instance\n",
    "    iterator: torch.utils.data.dataloader\n",
    "        a PyTorch dataloader\n",
    "    optimizer: torch.optim\n",
    "        a PyTorch optimizer \n",
    "    criterion: nn.CrossEntropyLoss\n",
    "        a PyTorch criterion \n",
    "\n",
    "    Return:\n",
    "    --------\n",
    "    float\n",
    "        average loss\n",
    "    \"\"\"\n",
    "    model.train()    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for img_features, captions in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # for each caption, the end word is not passed for training\n",
    "        outputs = model(\n",
    "            img_features.to(device),\n",
    "            captions[:, :-1].to(device)\n",
    "        )\n",
    "\n",
    "        loss = criterion(\n",
    "            outputs.view(-1, vocab_size), \n",
    "            captions[:, 1:].flatten().to(device)\n",
    "        )\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hpKuO9EPwcoc"
   },
   "outputs": [],
   "source": [
    "class SampleDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        descriptions,\n",
    "        imgs,\n",
    "        wordtoidx,\n",
    "        max_length\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes a SampleDataset\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        descriptions: list\n",
    "            a list of captions\n",
    "        imgs: numpy.ndarray\n",
    "            the image features\n",
    "        wordtoidx: dict\n",
    "            the dict to get word index\n",
    "        max_length: int\n",
    "            all captions will be padded to this size\n",
    "        \"\"\"        \n",
    "        self.imgs = imgs\n",
    "        self.descriptions = descriptions\n",
    "        self.wordtoidx = wordtoidx\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the batch size\n",
    "\n",
    "        Return:\n",
    "        --------\n",
    "        int\n",
    "            the batch size\n",
    "        \"\"\"\n",
    "        # return len(self.descriptions)\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Prepare data for each image\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        idx: int\n",
    "          the index of the image to process\n",
    "\n",
    "        Return:\n",
    "        --------\n",
    "        list, list, list\n",
    "            [5 x image feature matrix],\n",
    "            [five padded captions for this image]\n",
    "            [the length of each caption]\n",
    "        \"\"\"\n",
    "\n",
    "        img = self.imgs[idx // 5]\n",
    "        # convert each word into a list of sequences.\n",
    "        seq = [self.wordtoidx[word] for word \n",
    "               in self.descriptions[idx // 5][idx % 5].split(' ')\n",
    "               if word in self.wordtoidx]\n",
    "        # pad the sequence with 0 on the right side\n",
    "        in_seq = np.pad(\n",
    "            seq, \n",
    "            (0, max_length - len(seq)),\n",
    "            mode='constant',\n",
    "            constant_values=(0, 0)\n",
    "            )\n",
    "\n",
    "        return img, in_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_-mdGlavHoNi"
   },
   "outputs": [],
   "source": [
    "def init_weights(model, embedding_pretrained=True):\n",
    "    \"\"\"\n",
    "    Initialize weights and bias in the model\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    model: CaptionModel\n",
    "      a CaptionModel instance\n",
    "    embedding_pretrained: bool (default: True)\n",
    "        not initialize the embedding matrix if True\n",
    "    \"\"\"  \n",
    "  \n",
    "    for name, param in model.named_parameters():\n",
    "        if embedding_pretrained and 'embedding' in name:\n",
    "            continue\n",
    "        elif 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gYh9dzaSms1T"
   },
   "outputs": [],
   "source": [
    "def encode_image(model, img_path):\n",
    "    \"\"\"\n",
    "    Process the images to extract features\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    model: CNNModel\n",
    "      a CNNModel instance\n",
    "    img_path: str\n",
    "        the path of the image\n",
    " \n",
    "    Return:\n",
    "    --------\n",
    "    torch.Tensor\n",
    "        the extracted feature matrix from CNNModel\n",
    "    \"\"\"  \n",
    "\n",
    "    img = Image.open(img_path)\n",
    "\n",
    "    # Perform preprocessing needed by pre-trained models\n",
    "    preprocessor = transforms.Compose([\n",
    "        transforms.Resize(model.input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    img = preprocessor(img)\n",
    "    # Expand to 2D array\n",
    "    img = img.view(1, *img.shape)\n",
    "    # Call model to extract the smaller feature set for the image.\n",
    "    x = model(img.to(device), False) \n",
    "    # Shape to correct form to be accepted by LSTM captioning network.\n",
    "    x = np.squeeze(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n93pfnZhg1KX"
   },
   "outputs": [],
   "source": [
    "def extract_img_features(img_paths, file_path, model=None):\n",
    "    \"\"\"\n",
    "    Extracts, stores and returns image features\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    img_paths: list\n",
    "        the paths of images\n",
    "    file_path: str\n",
    "        the path to store the results\n",
    "    model: CNNModel (default: None)\n",
    "      a CNNModel instance\n",
    "\n",
    "    Return:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        the extracted image feature matrix from CNNModel\n",
    "    \"\"\" \n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        if model is None:\n",
    "            raise Exception(\"Please pass a CNNModel instance.\")\n",
    "        start = time()\n",
    "        img_features = []\n",
    "        \n",
    "        for image_path in tqdm(img_paths):\n",
    "            \n",
    "            img_features.append(\n",
    "                F.adaptive_avg_pool2d(\n",
    "                    (encode_image(model, image_path).cpu()), \n",
    "                    (1, 1)\n",
    "                ).squeeze().data.numpy()\n",
    "            )\n",
    "      \n",
    "        with open(file_path, \"wb\") as fp:\n",
    "            pickle.dump(img_features, fp)\n",
    "\n",
    "        print(f\"\\nGenerating set took: {hms_string(time()-start)}\")\n",
    "\n",
    "    else:\n",
    "        with open(file_path, \"rb\") as fp:\n",
    "            img_features = pickle.load(fp)\n",
    "\n",
    "    return img_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "a7f00f7c1fd54ce7812b4484816fc33f",
      "ade692e51d2a41e4b46b2d434721beae",
      "017f35aa2b8b4778b0f8ca18354302c5",
      "66c67f422cad491582d40cdf4c59684e",
      "11709de0e4604130a7e2178840fd2098",
      "06bb2cf3f4ad475fa285b68c151f2ca5",
      "b1cc3b64b08c485b96881af2e550dda8",
      "a6499c4a137f44bcb95086df80e4ba64"
     ]
    },
    "colab_type": "code",
    "id": "oqhZ7BPobdi1",
    "outputId": "f5d60d52-84d0-4c81-b643-d2efc68a030d"
   },
   "outputs": [],
   "source": [
    "# encoder = CNNModel(pretrained=True)\n",
    "# encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "1fTNcjLMh7sw",
    "outputId": "fa584b58-aa25-4079-8670-7b65fb0cda8a"
   },
   "outputs": [],
   "source": [
    "train_img_features = extract_img_features(\n",
    "    train_paths,\n",
    "    f'{root_captioning}/train_full_9.3.pkl',\n",
    "#     encoder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "NYiu8vKFoCak",
    "outputId": "415a2077-c0c2-4979-b4e3-ad37921a2f5e"
   },
   "outputs": [],
   "source": [
    "test_img_features = extract_img_features(\n",
    "    test_paths,\n",
    "    f'{root_captioning}/test_full_9.3.pkl',\n",
    "#     encoder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240
    },
    "colab_type": "code",
    "id": "Gy7bl0b3Z6Ul",
    "outputId": "46dc1503-d567-4271-8013-3544f2a67fe1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CaptionModel(\n",
       "  (decoder): RNNModel(\n",
       "    (embedding): Embedding(902, 200, padding_idx=0)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (lstm): LSTM(200, 256, batch_first=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (dense1): Linear(in_features=2048, out_features=256, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (dense2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (dense3): Linear(in_features=256, out_features=902, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_type = 'inception_v3'\n",
    "# cnn_type = 'vgg16'\n",
    "\n",
    "caption_model_1 = CaptionModel(\n",
    "    cnn_type, \n",
    "    vocab_size, \n",
    "    embedding_dim=200, \n",
    "    hidden_size=256,\n",
    "    embedding_matrix=None, \n",
    "    embedding_train=True\n",
    ")\n",
    "\n",
    "\n",
    "caption_model_1.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240
    },
    "colab_type": "code",
    "id": "Gy7bl0b3Z6Ul",
    "outputId": "46dc1503-d567-4271-8013-3544f2a67fe1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CaptionModel(\n",
       "  (decoder): RNNModel(\n",
       "    (embedding): Embedding(902, 200, padding_idx=0)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (lstm): LSTM(200, 256, batch_first=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (dense1): Linear(in_features=2048, out_features=256, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (dense2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (dense3): Linear(in_features=256, out_features=902, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption_model_2 = CaptionModel(\n",
    "    cnn_type, \n",
    "    vocab_size, \n",
    "    embedding_dim=embedding_dim, \n",
    "    hidden_size=256,\n",
    "    embedding_matrix=embedding_matrix, \n",
    "    embedding_train=True\n",
    ")\n",
    "\n",
    "caption_model_2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gze36nMIfoEg"
   },
   "outputs": [],
   "source": [
    "init_weights(\n",
    "    caption_model_1,\n",
    "    embedding_pretrained=False\n",
    ")\n",
    "\n",
    "## we will ignore the pad token in true target set\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = 0)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    caption_model_1.parameters(), \n",
    "    lr=0.01\n",
    ")\n",
    "\n",
    "train_dataset = SampleDataset(\n",
    "    train_descriptions,\n",
    "    train_img_features,\n",
    "    wordtoidx,\n",
    "    max_length\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "dyCHBbjnf2b1",
    "outputId": "df98da24-5d03-4917-bddc-5f04bf15ae80"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [00:00<00:19,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.151164531707764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [00:01<00:18,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.9246374236212835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [00:01<00:17,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.37503883573744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [00:02<00:16,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.90399874581231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [00:03<00:15,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.519996404647827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [00:03<00:15,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.154875569873386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [00:04<00:14,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8082154062059193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [00:05<00:13,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.537147707409329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 9/30 [00:05<00:13,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.35441878106859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [00:06<00:12,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.210924890306261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 11/30 [00:06<00:12,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.056483268737793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [00:07<00:11,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9270703925026789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 13/30 [00:08<00:10,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8229396608140733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 14/30 [00:08<00:09,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7350531419118245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 15/30 [00:09<00:09,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.623209555943807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 16/30 [00:10<00:08,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5320977634853787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 17/30 [00:10<00:08,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.45037399397956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18/30 [00:11<00:07,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3856452571021185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 19/30 [00:12<00:07,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3341154257456462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [00:12<00:07,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2879682646857367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 21/30 [00:13<00:06,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.258163485262129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 22/30 [00:14<00:05,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2057121528519525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 23/30 [00:15<00:05,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1571252478493586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24/30 [00:15<00:04,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1040045155419245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 25/30 [00:16<00:03,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.051502737734053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 26/30 [00:17<00:02,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0152224567201402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [00:17<00:02,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9884816077020433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 28/30 [00:18<00:01,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9693609144952562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 29/30 [00:19<00:00,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9389197892612882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:19<00:00,  1.52it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.892349534564548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [00:00<00:17,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8643178277545505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [00:01<00:17,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.839093440108829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [00:01<00:16,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8160745104153951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [00:02<00:16,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.803276903099484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [00:03<00:15,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7954957485198975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [00:03<00:14,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7900225851270888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [00:04<00:14,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7864181796709696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [00:05<00:14,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7854490809970431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 9/30 [00:05<00:14,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7795210944281684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [00:06<00:13,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7803593443499671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 11/30 [00:07<00:12,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7771336634953817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [00:07<00:11,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7787542045116425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 13/30 [00:08<00:10,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7744428416093191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 14/30 [00:08<00:10,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.773181762960222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 15/30 [00:09<00:09,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7707406116856469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 16/30 [00:10<00:08,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7695128354761336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 17/30 [00:10<00:08,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7695378197564019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18/30 [00:11<00:07,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7665311727258894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 19/30 [00:12<00:06,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.767546190155877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [00:12<00:06,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7646115256680382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 21/30 [00:13<00:05,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7651550239986844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 22/30 [00:13<00:05,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7630699541833665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 23/30 [00:14<00:04,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7631990909576416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24/30 [00:15<00:03,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7623770468764834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 25/30 [00:15<00:03,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7613599863317277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 26/30 [00:16<00:02,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.760589599609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [00:17<00:01,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7567144996590085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 28/30 [00:17<00:01,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7571767800384097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 29/30 [00:18<00:00,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.757081194056405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:19<00:00,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7569080822997623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clip = 1\n",
    "\n",
    "# model_path = f'{root_captioning}/caption-model_pytorch_inc_v3_9.3.hdf5'\n",
    "\n",
    "# if not os.path.exists(model_path):\n",
    "if True:\n",
    "    start = time()\n",
    "\n",
    "    for i in tqdm(range(EPOCHS * 3)):\n",
    "        \n",
    "        loss = train(caption_model_1, train_loader, optimizer, criterion, clip)\n",
    "        print(loss)\n",
    "        \n",
    "    # reduce the learning rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = 1e-4\n",
    "\n",
    "    for i in tqdm(range(EPOCHS * 3)):\n",
    "\n",
    "        loss = train(caption_model_1, train_loader, optimizer, criterion, clip)\n",
    "        print(loss)\n",
    "        \n",
    "#     torch.save(caption_model, model_path)\n",
    "#     print(f\"\\Training took: {hms_string(time()-start)}\")\n",
    "\n",
    "# else:\n",
    "#     caption_model = torch.load(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gze36nMIfoEg"
   },
   "outputs": [],
   "source": [
    "init_weights(\n",
    "    caption_model_2,\n",
    "    embedding_pretrained=True\n",
    ")\n",
    "\n",
    "## we will ignore the pad token in true target set\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = 0)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    caption_model_2.parameters(), \n",
    "    lr=0.01\n",
    ")\n",
    "\n",
    "train_dataset = SampleDataset(\n",
    "    train_descriptions,\n",
    "    train_img_features,\n",
    "    wordtoidx,\n",
    "    max_length\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "dyCHBbjnf2b1",
    "outputId": "df98da24-5d03-4917-bddc-5f04bf15ae80"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [00:00<00:18,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.026349226633708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [00:01<00:17,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.9601197772555885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [00:01<00:16,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.5973747041490345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [00:02<00:16,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.373509777916802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [00:03<00:15,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.957859913508097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [00:03<00:14,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4489263693491616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [00:04<00:14,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0525982644822864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [00:04<00:13,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7514285246531167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 9/30 [00:05<00:13,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.506771935356988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [00:06<00:12,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3021128707461886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 11/30 [00:06<00:11,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1413431432512073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [00:07<00:11,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.022365027003818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 13/30 [00:08<00:11,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9160572025511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 14/30 [00:08<00:10,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8122175931930542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 15/30 [00:09<00:09,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7173647085825603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 16/30 [00:10<00:08,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6399002605014377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 17/30 [00:10<00:08,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.56422135565016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18/30 [00:11<00:08,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4979095194074843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 19/30 [00:12<00:07,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4437693225012884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [00:13<00:07,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4049056768417358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 21/30 [00:13<00:06,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.364296509159936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 22/30 [00:14<00:05,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.32752439710829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 23/30 [00:15<00:05,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2900399896833632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24/30 [00:16<00:04,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.256981333096822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 25/30 [00:16<00:03,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2290719085269504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 26/30 [00:17<00:02,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2057688170009189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [00:18<00:02,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1816046370400324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 28/30 [00:18<00:01,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.141560243235694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 29/30 [00:19<00:00,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1081683900621202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:20<00:00,  1.49it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0810852977964613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [00:00<00:18,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0536504652765062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [00:01<00:17,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.029432521926032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [00:01<00:16,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0138022303581238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [00:02<00:16,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0014275776015387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [00:03<00:15,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9926450252532959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [00:03<00:14,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9901778830422295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [00:04<00:14,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.988030195236206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [00:04<00:13,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9834972553782992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 9/30 [00:05<00:13,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9811401698324416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [00:06<00:12,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9793292350239224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 11/30 [00:06<00:11,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.978055112891727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [00:07<00:11,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9795799718962775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 13/30 [00:08<00:10,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.976159950097402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 14/30 [00:08<00:10,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9729796118206449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 15/30 [00:09<00:09,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9748335414462619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 16/30 [00:09<00:08,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9729436702198453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 17/30 [00:10<00:08,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9708554281128777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18/30 [00:11<00:07,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9702145324812995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 19/30 [00:11<00:06,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9694207774268256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [00:12<00:06,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9681843386756049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 21/30 [00:13<00:05,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9665178457895914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 22/30 [00:13<00:04,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9665931132104661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 23/30 [00:14<00:04,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9662931031650968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24/30 [00:14<00:03,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9656590951813592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 25/30 [00:15<00:03,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9650584657986959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 26/30 [00:16<00:02,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9627931978967454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [00:16<00:01,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9626719156901041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 28/30 [00:17<00:01,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9593049089113871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 29/30 [00:18<00:00,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9597794413566589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:18<00:00,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9598200784789191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clip = 1\n",
    "\n",
    "# model_path = f'{root_captioning}/caption-model_pytorch_inc_v3_9.3.hdf5'\n",
    "\n",
    "# if not os.path.exists(model_path):\n",
    "if True:\n",
    "    start = time()\n",
    "\n",
    "    for i in tqdm(range(EPOCHS * 3)):\n",
    "        \n",
    "        loss = train(caption_model_2, train_loader, optimizer, criterion, clip)\n",
    "        print(loss)\n",
    "        \n",
    "    # reduce the learning rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = 1e-4\n",
    "\n",
    "    for i in tqdm(range(EPOCHS * 3)):\n",
    "\n",
    "        loss = train(caption_model_2, train_loader, optimizer, criterion, clip)\n",
    "        print(loss)\n",
    "        \n",
    "#     torch.save(caption_model, model_path)\n",
    "#     print(f\"\\Training took: {hms_string(time()-start)}\")\n",
    "\n",
    "# else:\n",
    "#     caption_model = torch.load(model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare randomly initialized embeddings learned from scratch and pre-trained embeddings trained on captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bLFJzXIK8sfp"
   },
   "outputs": [],
   "source": [
    "word_pairs = [('buildings','pools'), \n",
    "              ('ocean', 'beach'), \n",
    "              ('trees', 'green'), \n",
    "              ('several', 'road'), \n",
    "              ('planted', 'mountain'),\n",
    "              ('bridge', 'river'),\n",
    "              ('pond', 'desert')\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "colab_type": "code",
    "id": "oBjeWiQS82lh",
    "outputId": "75f5a539-3f99-467d-b836-05efca4898d5"
   },
   "outputs": [],
   "source": [
    "results = {'word1': [],\n",
    "           'word2': [],\n",
    "           'cosine similarity (learned from scratch)': [],\n",
    "           'cosine similarity (pre-trained)': []}\n",
    "for word in word_pairs:\n",
    "\n",
    "    results['word1'].append(word[0])\n",
    "    results['word2'].append(word[1])\n",
    "\n",
    "    results['cosine similarity (learned from scratch)']\\\n",
    "    .append(\n",
    "        np.squeeze(\n",
    "        cosine_similarity(\n",
    "            caption_model_1.decoder.embedding.weight.cpu().data.numpy()[wordtoidx[word[0]]].reshape(1, -1),\n",
    "            caption_model_1.decoder.embedding.weight.cpu().data.numpy()[wordtoidx[word[1]]].reshape(1, -1)\n",
    "        ))) \n",
    "    \n",
    "    results['cosine similarity (pre-trained)']\\\n",
    "    .append(\n",
    "        np.squeeze(\n",
    "        cosine_similarity(\n",
    "            caption_model_2.decoder.embedding.weight.cpu().data.numpy()[wordtoidx[word[0]]].reshape(1, -1),\n",
    "            caption_model_2.decoder.embedding.weight.cpu().data.numpy()[wordtoidx[word[1]]].reshape(1, -1)\n",
    "        )))\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>cosine similarity (learned from scratch)</th>\n",
       "      <th>cosine similarity (pre-trained)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>buildings</td>\n",
       "      <td>pools</td>\n",
       "      <td>0.5774896</td>\n",
       "      <td>0.290734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ocean</td>\n",
       "      <td>beach</td>\n",
       "      <td>0.23822954</td>\n",
       "      <td>0.4060871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trees</td>\n",
       "      <td>green</td>\n",
       "      <td>-0.15888813</td>\n",
       "      <td>0.087963894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>several</td>\n",
       "      <td>road</td>\n",
       "      <td>-0.051148847</td>\n",
       "      <td>-0.14882685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>planted</td>\n",
       "      <td>mountain</td>\n",
       "      <td>-0.11317037</td>\n",
       "      <td>0.11385666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bridge</td>\n",
       "      <td>river</td>\n",
       "      <td>0.33074567</td>\n",
       "      <td>0.45866707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pond</td>\n",
       "      <td>desert</td>\n",
       "      <td>0.073872365</td>\n",
       "      <td>0.2261277</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word1     word2 cosine similarity (learned from scratch)  \\\n",
       "0  buildings     pools                                0.5774896   \n",
       "1      ocean     beach                               0.23822954   \n",
       "2      trees     green                              -0.15888813   \n",
       "3    several      road                             -0.051148847   \n",
       "4    planted  mountain                              -0.11317037   \n",
       "5     bridge     river                               0.33074567   \n",
       "6       pond    desert                              0.073872365   \n",
       "\n",
       "  cosine similarity (pre-trained)  \n",
       "0                        0.290734  \n",
       "1                       0.4060871  \n",
       "2                     0.087963894  \n",
       "3                     -0.14882685  \n",
       "4                      0.11385666  \n",
       "5                      0.45866707  \n",
       "6                       0.2261277  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
