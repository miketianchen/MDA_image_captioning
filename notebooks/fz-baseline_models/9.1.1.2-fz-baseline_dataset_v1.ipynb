{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BaIK-bqzFhja"
   },
   "source": [
    "## Image Captioning with Pytorch\n",
    "\n",
    "The following contents are modified from MDS DSCI 575 lecture 8 demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m2p8iXQygqRN"
   },
   "outputs": [],
   "source": [
    "import os, sys, json\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import models, transforms, datasets\n",
    "from torchsummary import summary\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from nltk.translate import bleu_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "sys.path.append('../../scr/evaluation')\n",
    "from pycocoevalcap.tokenizer.ptbtokenizer import PTBTokenizer\n",
    "from pycocoevalcap.bleu.bleu import Bleu\n",
    "from pycocoevalcap.meteor.meteor import Meteor\n",
    "from pycocoevalcap.rouge.rouge import Rouge\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "from pycocoevalcap.spice.spice import Spice\n",
    "from pycocoevalcap.usc_sim.usc_sim import usc_sim\n",
    "import subprocess\n",
    "\n",
    "START = \"startseq\"\n",
    "STOP = \"endseq\"\n",
    "EPOCHS = 10\n",
    "AWS = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vmle0AXFBSdk"
   },
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "# import gc \n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "colab_type": "code",
    "id": "2W5oWBSofR7d",
    "outputId": "a0cc11ef-0e70-470c-e4b5-53912f21e341"
   },
   "outputs": [],
   "source": [
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return f\"{h}:{m:>02}:{s:>05.2f}\"\n",
    "        \n",
    "if AWS:\n",
    "    root_captioning = \"../../data\"\n",
    "else:\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive', force_remount=True)\n",
    "        root_captioning = \"/content/drive/My Drive/data\"\n",
    "        COLAB = True\n",
    "        print(\"Note: using Google CoLab\")\n",
    "    except:\n",
    "        print(\"Note: not using Google CoLab\")\n",
    "        COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VlBS8URRgaOo"
   },
   "source": [
    "### Clean/Build Dataset\n",
    "\n",
    "- Read captions\n",
    "- Preprocess captions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ra-d7BLlf1nC"
   },
   "outputs": [],
   "source": [
    "def get_img_info(name, num=np.inf):\n",
    "    \"\"\"\n",
    "    Returns img paths and captions\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    name: str\n",
    "        the json file name\n",
    "    num: int (default: np.inf)\n",
    "        the number of observations to get\n",
    "\n",
    "    Return:\n",
    "    --------\n",
    "    list, dict, int\n",
    "        img paths, corresponding captions, max length of captions\n",
    "    \"\"\"\n",
    "    img_path = []\n",
    "    caption = [] \n",
    "    max_length = 0\n",
    "    if AWS:\n",
    "        with open(f'{root_captioning}/json/{name}.json', 'r') as json_data:\n",
    "            data = json.load(json_data)\n",
    "            for filename in data.keys():\n",
    "                if num is not None and len(caption) == num:\n",
    "                    break\n",
    "                img_path.append(\n",
    "                    f'{root_captioning}/{name}/{filename}'\n",
    "                )\n",
    "                sen_list = []\n",
    "                for sentence in data[filename]['sentences']:\n",
    "                    max_length = max(max_length, len(sentence['tokens']))\n",
    "                    sen_list.append(sentence['raw'])\n",
    "\n",
    "                caption.append(sen_list)    \n",
    "    else:            \n",
    "        with open(f'{root_captioning}/interim/{name}.json', 'r') as json_data:\n",
    "            data = json.load(json_data)\n",
    "            for set_name in ['rsicd', 'ucm']:\n",
    "                for filename in data[set_name].keys():\n",
    "                    if num is not None and len(caption) == num:\n",
    "                        break\n",
    "\n",
    "                    img_path.append(\n",
    "                        f'{root_captioning}/raw/imgs/{set_name}/{filename}'\n",
    "                    )\n",
    "                    sen_list = []\n",
    "                    for sentence in data[set_name][filename]['sentences']:\n",
    "                        max_length = max(max_length, len(sentence['tokens']))\n",
    "                        sen_list.append(sentence['raw'])\n",
    "\n",
    "                    caption.append(sen_list)\n",
    "    \n",
    "    return img_path, caption, max_length            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zMUxfR9xKICI"
   },
   "outputs": [],
   "source": [
    "# get img path and caption list\n",
    "# # only test 800 train samples and 200 valid samples\n",
    "# train_paths, train_descriptions, max_length_train = get_img_info('train', 800)\n",
    "# test_paths, test_descriptions, max_length_test = get_img_info('valid', 200)\n",
    "\n",
    "train_paths, train_descriptions, max_length_train = get_img_info('train')\n",
    "valid_paths, valid_descriptions, max_length_valid = get_img_info('valid')\n",
    "test_paths, test_descriptions, max_length_test = get_img_info('test')\n",
    "sydney_paths, sydney_descriptions, max_length_sydney = get_img_info('sydney')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10416 images for training\n",
      "There are 2912 unique words (vocab)\n",
      "The maximum length of captions with start and stop token is 36.\n",
      "The maximum length of captions with start and stop token in test is 30.\n",
      "The maximum length of captions with start and stop token in the sydney dataset is 20.\n"
     ]
    }
   ],
   "source": [
    "train_paths.extend(valid_paths.copy())\n",
    "train_paths = np.array(train_paths)\n",
    "\n",
    "train_descriptions.extend(valid_descriptions.copy())\n",
    "train_descriptions = np.array(train_descriptions)\n",
    "\n",
    "captions = train_descriptions.copy()\n",
    "max_length_all = max(max_length_train, max_length_valid)\n",
    "max_length = max_length_all + 2\n",
    "      \n",
    "lex = set()\n",
    "for sen in train_descriptions:\n",
    "    [lex.update(d.split()) for d in sen]\n",
    "    \n",
    "# add a start and stop token at the beginning/end\n",
    "for v in train_descriptions:\n",
    "    for d in range(len(v)):\n",
    "        v[d] = f'{START} {v[d]} {STOP}'\n",
    "        \n",
    "print(f'There are {len(train_paths)} images for training') \n",
    "print(f'There are {len(lex)} unique words (vocab)')\n",
    "print(f'The maximum length of captions with start and stop token is {max_length}.')\n",
    "print(f'The maximum length of captions with start and stop token in test is {max_length_test}.')\n",
    "print(f'The maximum length of captions with start and stop token in the sydney dataset is {max_length_sydney}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2ZQ_Cl71YM55",
    "outputId": "93c0761e-95bf-4e7e-9313-a26106a48327"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../s3/valid/rsicd_park_33.jpg'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_paths[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "colab_type": "code",
    "id": "v3_PCjddHcUe",
    "outputId": "a8170e9a-51c0-44bc-94ea-d2a7eb8702ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['startseq a vast artificial lake was built in the park . endseq',\n",
       "       'startseq there are many residential areas near the park . endseq',\n",
       "       'startseq there are many residential areas near the park . endseq',\n",
       "       'startseq a vast artificial lake was built in the park . endseq',\n",
       "       'startseq a vast artificial lake was built in the park . endseq'],\n",
       "      dtype='<U184')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_descriptions[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vtIIFt4m6pTv"
   },
   "source": [
    "### Loading Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "IY_9XZ4Hec73",
    "outputId": "22d9061d-9070-4212-b7eb-989a9b7631a3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:22, 17819.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {} \n",
    "path = os.path.join(root_captioning, 'glove.6B.200d.txt') if AWS\\\n",
    "else os.path.join(root_captioning, 'raw', 'glove.6B.200d.txt')\n",
    "\n",
    "f = open(\n",
    "    path, \n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "\n",
    "f.close()\n",
    "print(f'Found {len(embeddings_index)} word vectors.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(descriptions, word_count_threshold=10):\n",
    "\n",
    "    captions = []\n",
    "    for val in descriptions:\n",
    "        for cap in val:\n",
    "            captions.append(cap)\n",
    "    print(f'There are {len(captions)} captions')\n",
    "    \n",
    "    word_counts = {}\n",
    "    nsents = 0\n",
    "    for sent in captions:\n",
    "        nsents += 1\n",
    "        for w in sent.split(' '):\n",
    "            word_counts[w] = word_counts.get(w, 0) + 1\n",
    "\n",
    "    vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
    "    print('preprocessed words %d ==> %d' % (len(word_counts), len(vocab)))\n",
    "    return vocab\n",
    "\n",
    "def get_word_dict(vocab):\n",
    "    \n",
    "    idxtoword = {}\n",
    "    wordtoidx = {}\n",
    "\n",
    "    ix = 1\n",
    "    for w in vocab:\n",
    "        wordtoidx[w] = ix\n",
    "        idxtoword[ix] = w\n",
    "        ix += 1\n",
    "\n",
    "    return idxtoword, wordtoidx\n",
    "\n",
    "def get_vocab_size(idxtoword):\n",
    "    \n",
    "    print(f'The vocabulary size is {len(idxtoword) + 1}.')\n",
    "    return len(idxtoword) + 1\n",
    "\n",
    "\n",
    "def get_embeddings(embeddings_index, vocab_size, embedding_dim, wordtoidx):\n",
    "\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    count =0\n",
    "\n",
    "    for word, i in wordtoidx.items():\n",
    "\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            count += 1\n",
    "            # Words not found in the embedding index will be all zeros\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    print(f'{count} out of {vocab_size} words are found in the pre-trained matrix.')            \n",
    "    print(f'The size of embedding_matrix is {embedding_matrix.shape}')\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kYH3-S2n82MN"
   },
   "source": [
    "### Building the Neural Network\n",
    "\n",
    "An embedding matrix is built from Glove.  This will be directly copied to the weight matrix of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "w33HKER-vZ7U",
    "outputId": "8c21728f-002c-4050-8a01-498dadf69f0d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DWOP58lrtGwt"
   },
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, cnn_type, pretrained=True):\n",
    "        \"\"\"\n",
    "        Initializes a CNNModel\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        cnn_type: str\n",
    "            the CNN type, either 'vgg16' or 'inception_v3'\n",
    "        pretrained: bool (default: True)\n",
    "            use pretrained model if True\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        super(CNNModel, self).__init__()\n",
    "\n",
    "        if cnn_type == 'vgg16':\n",
    "            self.model = models.vgg16(pretrained=pretrained)\n",
    "\n",
    "            # remove the last two layers in classifier\n",
    "            self.model.classifier = nn.Sequential(\n",
    "              *list(self.model.classifier.children())[:-2]\n",
    "            )\n",
    "            self.input_size = 224     \n",
    "\n",
    "        # inception v3 expects (299, 299) sized images\n",
    "        elif cnn_type == 'inception_v3':\n",
    "            self.model = models.inception_v3(pretrained=pretrained)\n",
    "            # remove the classification layer\n",
    "            self.model.fc = nn.Identity()\n",
    "\n",
    "            # turn off auxiliary output\n",
    "            self.model.aux_logits = False\n",
    "            self.input_size = 299\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"Please choose between 'vgg16' and 'inception_v3'.\")\n",
    "\n",
    "    def forward(self, img_input, train=False):\n",
    "        \"\"\"\n",
    "        forward of the CNNModel\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        img_input: torch.Tensor\n",
    "            the image matrix\n",
    "        train: bool (default: False)\n",
    "            use the model only for feature extraction if False\n",
    "\n",
    "        Return:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            image feature matrix\n",
    "        \"\"\"\n",
    "        if not train:\n",
    "            # set the model to evaluation model\n",
    "            self.model.eval()\n",
    "\n",
    "        return self.model(img_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5uWZ8QOOXtUM"
   },
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size, \n",
    "        embedding_dim, \n",
    "        hidden_size=256,\n",
    "        embedding_matrix=None, \n",
    "        embedding_train=False\n",
    "    ):\n",
    "      \n",
    "        \"\"\"\n",
    "        Initializes a RNNModel\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        vocab_size: int\n",
    "            the size of the vocabulary\n",
    "        embedding_dim: int\n",
    "            the number of features in the embedding matrix\n",
    "        hidden_size: int (default: 256)\n",
    "            the size of the hidden state in LSTM\n",
    "        embedding_matrix: torch.Tensor (default: None)\n",
    "            if not None, use this matrix as the embedding matrix\n",
    "        embedding_train: bool (default: False)\n",
    "            not train the embedding matrix if False\n",
    "        \"\"\"\n",
    "\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "\n",
    "        if embedding_matrix is not None:\n",
    "\n",
    "            self.embedding.load_state_dict({\n",
    "              'weight': torch.FloatTensor(embedding_matrix)\n",
    "            })\n",
    "            self.embedding.weight.requires_grad = embedding_train\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
    " \n",
    "\n",
    "    def forward(self, captions):\n",
    "        \"\"\"\n",
    "        forward of the RNNModel\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        captions: torch.Tensor\n",
    "            the padded caption matrix\n",
    "\n",
    "        Return:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            word probabilities for each position\n",
    "        \"\"\"\n",
    "\n",
    "        # embed the captions\n",
    "        embedding = self.dropout(self.embedding(captions))\n",
    "\n",
    "        outputs, (h, c) = self.lstm(embedding)\n",
    "\n",
    "        return outputs, (h, c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vzigDNU6jEly"
   },
   "outputs": [],
   "source": [
    "class CaptionModel(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        cnn_type, \n",
    "        vocab_size, \n",
    "        embedding_dim, \n",
    "        hidden_size=256,\n",
    "        embedding_matrix=None, \n",
    "        embedding_train=False\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        Initializes a CaptionModel\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        cnn_type: str\n",
    "            the CNN type, either 'vgg16' or 'inception_v3'\n",
    "        vocab_size: int\n",
    "            the size of the vocabulary\n",
    "        embedding_dim: int\n",
    "            the number of features in the embedding matrix\n",
    "        feature_size: int\n",
    "            the number of features in the image matrix\n",
    "        hidden_size: int (default: 256)\n",
    "            the size of the hidden state in LSTM\n",
    "        embedding_matrix: torch.Tensor (default: None)\n",
    "            if not None, use this matrix as the embedding matrix\n",
    "        embedding_train: bool (default: False)\n",
    "            not train the embedding matrix if False\n",
    "        \"\"\"    \n",
    "        super(CaptionModel, self).__init__() \n",
    "\n",
    "        # set feature_size based on cnn_type\n",
    "        if cnn_type == 'vgg16':\n",
    "            self.feature_size = 4096\n",
    "        elif cnn_type == 'inception_v3':\n",
    "            self.feature_size = 2048\n",
    "        else:\n",
    "            raise Exception(\"Please choose between 'vgg16' and 'inception_v3'.\")  \n",
    "\n",
    "        self.decoder = RNNModel(\n",
    "            vocab_size, \n",
    "            embedding_dim,\n",
    "            hidden_size,\n",
    "            embedding_matrix,\n",
    "            embedding_train\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.dense1 = nn.Linear(self.feature_size, hidden_size) \n",
    "        self.relu1 = nn.ReLU()\n",
    "          \n",
    "        self.dense2 = nn.Linear(hidden_size, hidden_size) \n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dense3 = nn.Linear(hidden_size, vocab_size) \n",
    "\n",
    "    def forward(self, img_features, captions):\n",
    "        \"\"\"\n",
    "        forward of the CaptionModel\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        img_features: torch.Tensor\n",
    "            the image feature matrix\n",
    "        captions: torch.Tensor\n",
    "            the padded caption matrix\n",
    "\n",
    "        Return:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            word probabilities for each position\n",
    "        \"\"\"\n",
    "\n",
    "        img_features =\\\n",
    "        self.relu1(\n",
    "            self.dense1(\n",
    "                self.dropout(\n",
    "                    img_features\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        decoder_out, _ = self.decoder(captions)\n",
    "\n",
    "        # add up decoder outputs and image features\n",
    "        outputs =\\\n",
    "        self.dense3(\n",
    "            self.relu2(\n",
    "                self.dense2(\n",
    "                    decoder_out.add(\n",
    "                        (img_features.view(img_features.size(0), 1, -1))\\\n",
    "                        .repeat(1, decoder_out.size(1), 1)\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wCSzCGtB8-KM"
   },
   "source": [
    "### Train the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R0ihg_mBrF-b"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip, vocab_size):\n",
    "    \"\"\"\n",
    "    train the CaptionModel\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    model: CaptionModel\n",
    "        a CaptionModel instance\n",
    "    iterator: torch.utils.data.dataloader\n",
    "        a PyTorch dataloader\n",
    "    optimizer: torch.optim\n",
    "        a PyTorch optimizer \n",
    "    criterion: nn.CrossEntropyLoss\n",
    "        a PyTorch criterion \n",
    "\n",
    "    Return:\n",
    "    --------\n",
    "    float\n",
    "        average loss\n",
    "    \"\"\"\n",
    "    model.train()    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for img_features, captions in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # for each caption, the end word is not passed for training\n",
    "        outputs = model(\n",
    "            img_features.to(device),\n",
    "            captions[:, :-1].to(device)\n",
    "        )\n",
    "\n",
    "        loss = criterion(\n",
    "            outputs.view(-1, vocab_size), \n",
    "            captions[:, 1:].flatten().to(device)\n",
    "        )\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hpKuO9EPwcoc"
   },
   "outputs": [],
   "source": [
    "class SampleDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        descriptions,\n",
    "        imgs,\n",
    "        wordtoidx,\n",
    "        max_length\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Initializes a SampleDataset\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        descriptions: list\n",
    "            a list of captions\n",
    "        imgs: numpy.ndarray\n",
    "            the image features\n",
    "        wordtoidx: dict\n",
    "            the dict to get word index\n",
    "        max_length: int\n",
    "            all captions will be padded to this size\n",
    "        \"\"\"        \n",
    "        self.imgs = imgs\n",
    "        self.descriptions = descriptions\n",
    "        self.wordtoidx = wordtoidx\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the batch size\n",
    "\n",
    "        Return:\n",
    "        --------\n",
    "        int\n",
    "            the batch size\n",
    "        \"\"\"\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Prepare data for each image\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        idx: int\n",
    "          the index of the image to process\n",
    "\n",
    "        Return:\n",
    "        --------\n",
    "        list, list, list\n",
    "            [5 x image feature matrix],\n",
    "            [five padded captions for this image]\n",
    "            [the length of each caption]\n",
    "        \"\"\"\n",
    "\n",
    "        img = self.imgs[idx]\n",
    "        img_features, captions = [], []\n",
    "        for desc in self.descriptions[idx]:\n",
    "            # convert each word into a list of sequences.\n",
    "            seq = [self.wordtoidx[word] for word in desc.split(' ')\n",
    "                  if word in self.wordtoidx]\n",
    "            # pad the sequence with 0 on the right side\n",
    "            in_seq = np.pad(\n",
    "                seq, \n",
    "                (0, max_length - len(seq)),\n",
    "                mode='constant',\n",
    "                constant_values=(0, 0)\n",
    "                )\n",
    "\n",
    "            img_features.append(img)\n",
    "            captions.append(in_seq)\n",
    "    \n",
    "        return (img_features, captions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qS7cHjnwq14b"
   },
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    \"\"\"\n",
    "    Processes the batch to return from the dataloader\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    batch: tuple\n",
    "      a batch from the Dataset\n",
    "\n",
    "    Return:\n",
    "    --------\n",
    "    list\n",
    "        [image feature matrix, captions, the length of each caption]\n",
    "    \"\"\"  \n",
    "\n",
    "    img_features = [item[0] for item in batch]\n",
    "    captions = [item[1] for item in batch]\n",
    "\n",
    "    img_features = torch.FloatTensor(list(chain(*img_features)))\n",
    "    captions = torch.LongTensor(list(chain(*captions)))\n",
    "\n",
    "    return [img_features, captions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_-mdGlavHoNi"
   },
   "outputs": [],
   "source": [
    "def init_weights(model, embedding_pretrained=True):\n",
    "    \"\"\"\n",
    "    Initialize weights and bias in the model\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    model: CaptionModel\n",
    "      a CaptionModel instance\n",
    "    embedding_pretrained: bool (default: True)\n",
    "        not initialize the embedding matrix if True\n",
    "    \"\"\"  \n",
    "  \n",
    "    for name, param in model.named_parameters():\n",
    "        if embedding_pretrained and 'embedding' in name:\n",
    "            continue\n",
    "        elif 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gYh9dzaSms1T"
   },
   "outputs": [],
   "source": [
    "def encode_image(model, img_path):\n",
    "    \"\"\"\n",
    "    Process the images to extract features\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    model: CNNModel\n",
    "      a CNNModel instance\n",
    "    img_path: str\n",
    "        the path of the image\n",
    " \n",
    "    Return:\n",
    "    --------\n",
    "    torch.Tensor\n",
    "        the extracted feature matrix from CNNModel\n",
    "    \"\"\"  \n",
    "\n",
    "    img = Image.open(img_path)\n",
    "\n",
    "    # Perform preprocessing needed by pre-trained models\n",
    "    preprocessor = transforms.Compose([\n",
    "        transforms.Resize(model.input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    img = preprocessor(img)\n",
    "    # Expand to 2D array\n",
    "    img = img.view(1, *img.shape)\n",
    "    # Call model to extract the smaller feature set for the image.\n",
    "    x = model(img.to(device), False) \n",
    "    # Shape to correct form to be accepted by LSTM captioning network.\n",
    "    x = np.squeeze(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n93pfnZhg1KX"
   },
   "outputs": [],
   "source": [
    "def extract_img_features(img_paths, model):\n",
    "    \"\"\"\n",
    "    Extracts, stores and returns image features\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    img_paths: list\n",
    "        the paths of images\n",
    "    model: CNNModel (default: None)\n",
    "      a CNNModel instance\n",
    "\n",
    "    Return:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        the extracted image feature matrix from CNNModel\n",
    "    \"\"\" \n",
    "\n",
    "    start = time()\n",
    "    img_features = []\n",
    "\n",
    "    for image_path in img_paths:\n",
    "        img_features.append(\n",
    "            encode_image(model, image_path).cpu().data.numpy()\n",
    "    )\n",
    "\n",
    "    print(f\"\\nGenerating set took: {hms_string(time()-start)}\")\n",
    "\n",
    "    return img_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fTNcjLMh7sw"
   },
   "outputs": [],
   "source": [
    "def get_train_test(\n",
    "    encoder,\n",
    "    train_paths,\n",
    "    test_paths,\n",
    "    sydney_paths\n",
    "):\n",
    "\n",
    "    train_img_features = extract_img_features(\n",
    "        train_paths,\n",
    "        encoder\n",
    "    )\n",
    "\n",
    "    test_img_features = extract_img_features(\n",
    "        test_paths,\n",
    "        encoder\n",
    "    )\n",
    "    \n",
    "    sydney_img_features = extract_img_features(\n",
    "        sydney_paths,\n",
    "        encoder\n",
    "    )\n",
    "    \n",
    "    return train_img_features, test_img_features, sydney_img_features\n",
    "\n",
    "def get_train_dataloader(\n",
    "    train_descriptions, \n",
    "    train_img_features,\n",
    "    wordtoidx,\n",
    "    max_length,\n",
    "    batch_size=200\n",
    "):\n",
    "    train_dataset = SampleDataset(\n",
    "        train_descriptions,\n",
    "        train_img_features,\n",
    "        wordtoidx,\n",
    "        max_length\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size,\n",
    "        collate_fn=my_collate\n",
    "    )\n",
    "    \n",
    "    return train_loader\n",
    "\n",
    "def train_model(\n",
    "    train_loader,\n",
    "    vocab_size,\n",
    "    embedding_dim, \n",
    "    embedding_matrix,\n",
    "    cnn_type='inception_v3',\n",
    "    hidden_size=256,\n",
    "):\n",
    "\n",
    "    caption_model = CaptionModel(\n",
    "        cnn_type, \n",
    "        vocab_size, \n",
    "        embedding_dim, \n",
    "        hidden_size=hidden_size,\n",
    "        embedding_matrix=embedding_matrix, \n",
    "        embedding_train=True\n",
    "    )\n",
    "\n",
    "    init_weights(\n",
    "        caption_model,\n",
    "        embedding_pretrained=True\n",
    "    )\n",
    "\n",
    "    caption_model.to(device)\n",
    "\n",
    "    # we will ignore the pad token in true target set\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        caption_model.parameters(), \n",
    "        lr=0.01\n",
    "    )\n",
    "\n",
    "    clip = 1\n",
    "    start = time()\n",
    "\n",
    "    for i in tqdm(range(EPOCHS * 6)):\n",
    "\n",
    "        loss = train(caption_model, train_loader, optimizer, criterion, clip, vocab_size)\n",
    "        print(loss)\n",
    "\n",
    "    # reduce the learning rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = 1e-4\n",
    "\n",
    "    for i in tqdm(range(EPOCHS * 6)):\n",
    "\n",
    "        loss = train(caption_model, train_loader, optimizer, criterion, clip, vocab_size)\n",
    "        print(loss)\n",
    "    return caption_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a5WoWWbc9Y-g"
   },
   "outputs": [],
   "source": [
    "def generateCaption(\n",
    "    model, \n",
    "    img_features,\n",
    "    max_length,\n",
    "    vocab_size,\n",
    "    wordtoidx,\n",
    "    idxtoword\n",
    "):\n",
    "    in_text = START\n",
    "\n",
    "    for i in range(max_length):\n",
    "\n",
    "        sequence = [wordtoidx[w] for w in in_text.split() if w in wordtoidx]\n",
    "        sequence = np.pad(sequence, (0, max_length - len(sequence)),\n",
    "                          mode='constant', constant_values=(0, 0))\n",
    "        model.eval()\n",
    "        yhat = model(\n",
    "            torch.FloatTensor(img_features)\\\n",
    "            .view(-1, model.feature_size).to(device),\n",
    "            torch.LongTensor(sequence).view(-1, max_length).to(device)\n",
    "        )\n",
    "\n",
    "        yhat = yhat.view(-1, vocab_size).argmax(1)\n",
    "        word = idxtoword[yhat.cpu().data.numpy()[i]]\n",
    "        in_text += ' ' + word\n",
    "        if word == STOP:\n",
    "            break\n",
    "    final = in_text.split()\n",
    "    final = final[1 : -1]\n",
    "    final = ' '.join(final)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_model(ref_data, results):\n",
    "    \"\"\"\n",
    "    Computes evaluation metrics of the model results against the human annotated captions\n",
    "    \n",
    "    Parameters:\n",
    "    ------------\n",
    "    ref_data: dict\n",
    "        a dictionary containing human annotated captions, with image name as key and a list of human annotated captions as values\n",
    "    \n",
    "    results: dict\n",
    "        a dictionary containing model generated caption, with image name as key and a generated caption as value\n",
    "        \n",
    "    Returns:\n",
    "    ------------\n",
    "    score_dict: a dictionary containing the overall average score for the model\n",
    "    \"\"\"\n",
    "    # download stanford nlp library\n",
    "    subprocess.call(['../../scr/evaluation/get_stanford_models.sh'])\n",
    "    \n",
    "    # format the inputs\n",
    "    gts = {}\n",
    "    res = {}\n",
    "\n",
    "    for imgId in range(len(ref_data)):\n",
    "        caption_list_sel = []\n",
    "        for i in range(5):\n",
    "            lst = {}\n",
    "            lst['caption'] = ref_data[imgId][i]\n",
    "            lst['image_id'] = imgId\n",
    "            lst['id'] = i\n",
    "            caption_list_sel.append(lst)\n",
    "        gts[imgId] = caption_list_sel\n",
    "\n",
    "        res[imgId] = [{'caption': results[imgId]}]\n",
    "        \n",
    "    # tokenize\n",
    "    print('tokenization...')\n",
    "    tokenizer = PTBTokenizer()\n",
    "    gts  = tokenizer.tokenize(gts)\n",
    "    res = tokenizer.tokenize(res)\n",
    "    \n",
    "    # compute scores\n",
    "    scorers = [\n",
    "        (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"]),\n",
    "        (Meteor(),\"METEOR\"),\n",
    "        (Rouge(), \"ROUGE_L\"),\n",
    "        (Cider(), \"CIDEr\"),\n",
    "        (Spice(), \"SPICE\"),\n",
    "        (usc_sim(), \"USC_similarity\"),  \n",
    "        ]\n",
    "    score_dict = {}\n",
    "    for scorer, method in scorers:\n",
    "        print('computing %s score...'%(scorer.method()))\n",
    "        score, scores = scorer.compute_score(gts, res)\n",
    "        if type(method) == list:\n",
    "            for sc, scs, m in zip(score, scores, method):\n",
    "                score_dict[m] = sc\n",
    "        else:\n",
    "            score_dict[method] = score\n",
    "            \n",
    "    return score_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_results(\n",
    "    test_img_features, \n",
    "    model,\n",
    "    ref,\n",
    "    max_length,\n",
    "    vocab_size,\n",
    "    wordtoidx,\n",
    "    idxtoword\n",
    "):\n",
    "    # generate results\n",
    "    print('Generating captions...')\n",
    "    results = {}\n",
    "    for n in range(len(test_img_features)):\n",
    "        img_features = test_img_features[n]\n",
    "        generated = generateCaption(\n",
    "            model, \n",
    "            img_features,\n",
    "            max_length,\n",
    "            vocab_size,\n",
    "            wordtoidx,\n",
    "            idxtoword\n",
    "        )\n",
    "        results[n] = generated\n",
    "        \n",
    "    model_score = eval_model(ref, results)\n",
    "\n",
    "    return model_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MAv_Bab17WeD"
   },
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNModel(\n",
       "  (model): Inception3(\n",
       "    (Conv2d_1a_3x3): BasicConv2d(\n",
       "      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (Conv2d_2a_3x3): BasicConv2d(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (Conv2d_2b_3x3): BasicConv2d(\n",
       "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (Conv2d_3b_1x1): BasicConv2d(\n",
       "      (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (Conv2d_4a_3x3): BasicConv2d(\n",
       "      (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (Mixed_5b): InceptionA(\n",
       "      (branch1x1): BasicConv2d(\n",
       "        (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch5x5_1): BasicConv2d(\n",
       "        (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch5x5_2): BasicConv2d(\n",
       "        (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3dbl_1): BasicConv2d(\n",
       "        (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3dbl_2): BasicConv2d(\n",
       "        (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3dbl_3): BasicConv2d(\n",
       "        (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch_pool): BasicConv2d(\n",
       "        (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (Mixed_5c): InceptionA(\n",
       "      (branch1x1): BasicConv2d(\n",
       "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch5x5_1): BasicConv2d(\n",
       "        (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch5x5_2): BasicConv2d(\n",
       "        (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3dbl_1): BasicConv2d(\n",
       "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3dbl_2): BasicConv2d(\n",
       "        (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3dbl_3): BasicConv2d(\n",
       "        (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch_pool): BasicConv2d(\n",
       "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (Mixed_5d): InceptionA(\n",
       "      (branch1x1): BasicConv2d(\n",
       "        (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch5x5_1): BasicConv2d(\n",
       "        (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch5x5_2): BasicConv2d(\n",
       "        (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3dbl_1): BasicConv2d(\n",
       "        (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3dbl_2): BasicConv2d(\n",
       "        (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3dbl_3): BasicConv2d(\n",
       "        (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch_pool): BasicConv2d(\n",
       "        (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (Mixed_6a): InceptionB(\n",
       "      (branch3x3): BasicConv2d(\n",
       "        (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3dbl_1): BasicConv2d(\n",
       "        (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3dbl_2): BasicConv2d(\n",
       "        (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3dbl_3): BasicConv2d(\n",
       "        (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (Mixed_6b): InceptionC(\n",
       "      (branch1x1): BasicConv2d(\n",
       "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7_1): BasicConv2d(\n",
       "        (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7_2): BasicConv2d(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7_3): BasicConv2d(\n",
       "        (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7dbl_1): BasicConv2d(\n",
       "        (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7dbl_2): BasicConv2d(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7dbl_3): BasicConv2d(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7dbl_4): BasicConv2d(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7dbl_5): BasicConv2d(\n",
       "        (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch_pool): BasicConv2d(\n",
       "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (Mixed_6c): InceptionC(\n",
       "      (branch1x1): BasicConv2d(\n",
       "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7_1): BasicConv2d(\n",
       "        (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7_2): BasicConv2d(\n",
       "        (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7_3): BasicConv2d(\n",
       "        (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7dbl_1): BasicConv2d(\n",
       "        (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7dbl_2): BasicConv2d(\n",
       "        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7dbl_3): BasicConv2d(\n",
       "        (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7dbl_4): BasicConv2d(\n",
       "        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7dbl_5): BasicConv2d(\n",
       "        (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch_pool): BasicConv2d(\n",
       "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (Mixed_6d): InceptionC(\n",
       "      (branch1x1): BasicConv2d(\n",
       "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7_1): BasicConv2d(\n",
       "        (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7_2): BasicConv2d(\n",
       "        (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7_3): BasicConv2d(\n",
       "        (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7dbl_1): BasicConv2d(\n",
       "        (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7dbl_2): BasicConv2d(\n",
       "        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7dbl_3): BasicConv2d(\n",
       "        (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7dbl_4): BasicConv2d(\n",
       "        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7dbl_5): BasicConv2d(\n",
       "        (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch_pool): BasicConv2d(\n",
       "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (Mixed_6e): InceptionC(\n",
       "      (branch1x1): BasicConv2d(\n",
       "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7_1): BasicConv2d(\n",
       "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7_2): BasicConv2d(\n",
       "        (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7_3): BasicConv2d(\n",
       "        (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7dbl_1): BasicConv2d(\n",
       "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7dbl_2): BasicConv2d(\n",
       "        (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7dbl_3): BasicConv2d(\n",
       "        (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7dbl_4): BasicConv2d(\n",
       "        (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7dbl_5): BasicConv2d(\n",
       "        (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch_pool): BasicConv2d(\n",
       "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (AuxLogits): InceptionAux(\n",
       "      (conv0): BasicConv2d(\n",
       "        (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): BasicConv2d(\n",
       "        (conv): Conv2d(128, 768, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (fc): Linear(in_features=768, out_features=1000, bias=True)\n",
       "    )\n",
       "    (Mixed_7a): InceptionD(\n",
       "      (branch3x3_1): BasicConv2d(\n",
       "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3_2): BasicConv2d(\n",
       "        (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7x3_1): BasicConv2d(\n",
       "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7x3_2): BasicConv2d(\n",
       "        (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7x3_3): BasicConv2d(\n",
       "        (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch7x7x3_4): BasicConv2d(\n",
       "        (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (Mixed_7b): InceptionE(\n",
       "      (branch1x1): BasicConv2d(\n",
       "        (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3_1): BasicConv2d(\n",
       "        (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3_2a): BasicConv2d(\n",
       "        (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3_2b): BasicConv2d(\n",
       "        (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3dbl_1): BasicConv2d(\n",
       "        (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3dbl_2): BasicConv2d(\n",
       "        (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3dbl_3a): BasicConv2d(\n",
       "        (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3dbl_3b): BasicConv2d(\n",
       "        (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch_pool): BasicConv2d(\n",
       "        (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (Mixed_7c): InceptionE(\n",
       "      (branch1x1): BasicConv2d(\n",
       "        (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3_1): BasicConv2d(\n",
       "        (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3_2a): BasicConv2d(\n",
       "        (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3_2b): BasicConv2d(\n",
       "        (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3dbl_1): BasicConv2d(\n",
       "        (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3dbl_2): BasicConv2d(\n",
       "        (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3dbl_3a): BasicConv2d(\n",
       "        (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch3x3dbl_3b): BasicConv2d(\n",
       "        (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (branch_pool): BasicConv2d(\n",
       "        (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (fc): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_type = 'inception_v3'\n",
    "encoder = CNNModel(cnn_type, pretrained=True)\n",
    "encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10416 images for training and 2605 images for testing.\n",
      "There are 52080 captions\n",
      "preprocessed words 2917 ==> 991\n",
      "The vocabulary size is 992.\n",
      "890 out of 992 words are found in the pre-trained matrix.\n",
      "The size of embedding_matrix is (992, 200)\n",
      "Preparing dataloader...\n",
      "\n",
      "Generating set took: 0:03:59.39\n",
      "\n",
      "Generating set took: 0:01:00.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating set took: 0:00:14.49\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 1/60 [00:17<17:17, 17.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.125539046413493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|         | 2/60 [00:35<17:08, 17.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4349821693492384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|         | 3/60 [00:53<16:55, 17.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8655062261617408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|         | 4/60 [01:11<16:39, 17.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6285990544085234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|         | 5/60 [01:29<16:23, 17.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5081218233648337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|         | 6/60 [01:47<16:06, 17.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4283225873731218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|        | 7/60 [02:05<15:49, 17.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3748359803883534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|        | 8/60 [02:23<15:32, 17.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.334864466820123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|        | 9/60 [02:41<15:16, 17.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.310101809366694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|        | 10/60 [02:59<15:02, 18.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2872075213576264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|        | 11/60 [03:17<14:45, 18.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2598161629910738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|        | 12/60 [03:36<14:29, 18.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.240003063993634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|       | 13/60 [03:54<14:11, 18.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2222710960316208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|       | 14/60 [04:12<13:52, 18.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.207795111638195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|       | 15/60 [04:30<13:34, 18.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1962268172569994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|       | 16/60 [04:48<13:15, 18.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1848198242907255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|       | 17/60 [05:06<12:58, 18.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1762961036754105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|       | 18/60 [05:24<12:40, 18.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1698689989323885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|      | 19/60 [05:42<12:21, 18.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1585932801354606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|      | 20/60 [06:00<12:03, 18.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.151490458902323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|      | 21/60 [06:18<11:45, 18.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1446466389692054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|      | 22/60 [06:36<11:27, 18.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.137423741929936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|      | 23/60 [06:55<11:10, 18.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1315647779770617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|      | 24/60 [07:13<10:52, 18.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1263626684557717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|     | 25/60 [07:31<10:34, 18.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1193546399755299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|     | 26/60 [07:49<10:17, 18.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.108877567750103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|     | 27/60 [08:07<09:59, 18.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.099726472823125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|     | 28/60 [08:25<09:41, 18.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0938788826735515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|     | 29/60 [08:44<09:22, 18.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0889354022044055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|     | 30/60 [09:02<09:04, 18.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.081164706990404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|    | 31/60 [09:20<08:47, 18.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0807407138482579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|    | 32/60 [09:38<08:28, 18.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.076981025484373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|    | 33/60 [09:57<08:12, 18.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0732109074322682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|    | 34/60 [10:15<07:55, 18.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.072811834654718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|    | 35/60 [10:33<07:36, 18.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0674128937271405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|    | 36/60 [10:52<07:18, 18.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0652910977039698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|   | 37/60 [11:10<07:00, 18.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0626761857068763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|   | 38/60 [11:28<06:42, 18.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0553761304549452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|   | 39/60 [11:46<06:23, 18.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0530390992479504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|   | 40/60 [12:05<06:05, 18.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.050795021484483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|   | 41/60 [12:23<05:47, 18.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0458384381150299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|   | 42/60 [12:41<05:29, 18.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0459435919545732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|  | 43/60 [13:00<05:11, 18.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0417945098202184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|  | 44/60 [13:18<04:52, 18.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0383040955606497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|  | 45/60 [13:36<04:34, 18.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0389495182712123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|  | 46/60 [13:55<04:16, 18.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0369378873762094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|  | 47/60 [14:13<03:58, 18.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0364152395500328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|  | 48/60 [14:31<03:40, 18.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0347336513816185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%| | 49/60 [14:50<03:21, 18.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0322997749976393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%| | 50/60 [15:08<03:03, 18.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0298514197457511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%| | 51/60 [15:26<02:45, 18.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0300844446668085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%| | 52/60 [15:45<02:26, 18.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.027974711274201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%| | 53/60 [16:03<02:08, 18.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0247956233204536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%| | 54/60 [16:21<01:49, 18.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0282179780726164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|| 55/60 [16:40<01:31, 18.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0217548271395125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|| 56/60 [16:58<01:13, 18.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0210376499958758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|| 57/60 [17:16<00:54, 18.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.018811156727233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|| 58/60 [17:34<00:36, 18.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0192456222930044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|| 59/60 [17:53<00:18, 18.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0231648250570837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [18:11<00:00, 18.19s/it]\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0203262663112496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 1/60 [00:18<18:04, 18.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9892684626129439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|         | 2/60 [00:36<17:45, 18.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9639798076647632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|         | 3/60 [00:55<17:27, 18.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9547862629845457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|         | 4/60 [01:13<17:08, 18.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9488548338413239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|         | 5/60 [01:31<16:49, 18.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9438309776333144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|         | 6/60 [01:50<16:29, 18.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.941699559396168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|        | 7/60 [02:08<16:13, 18.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9384333935548674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|        | 8/60 [02:26<15:56, 18.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9370078489465533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|        | 9/60 [02:45<15:39, 18.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9345929870065653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|        | 10/60 [03:03<15:21, 18.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9310146154097791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|        | 11/60 [03:22<15:04, 18.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9296032274669072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|        | 12/60 [03:40<14:46, 18.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9281603800800612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|       | 13/60 [03:59<14:27, 18.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.927061233318077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|       | 14/60 [04:17<14:09, 18.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9251874629056679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|       | 15/60 [04:36<13:51, 18.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9258232454084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|       | 16/60 [04:54<13:33, 18.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9252675406213077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|       | 17/60 [05:13<13:14, 18.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9226629542854597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|       | 18/60 [05:31<12:55, 18.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9215360469413254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|      | 19/60 [05:50<12:37, 18.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9205863993122892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|      | 20/60 [06:08<12:18, 18.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9202160795904556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|      | 21/60 [06:27<12:00, 18.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9188193858794447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|      | 22/60 [06:45<11:43, 18.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.917858971177407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|      | 23/60 [07:04<11:25, 18.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9166223243722376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|      | 24/60 [07:22<11:06, 18.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.916517122736517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|     | 25/60 [07:41<10:47, 18.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9153531418656403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|     | 26/60 [07:59<10:28, 18.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.915320707941955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|     | 27/60 [08:18<10:09, 18.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9145653883241257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|     | 28/60 [08:36<09:48, 18.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.913578321348946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|     | 29/60 [08:54<09:28, 18.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9121758099996818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|     | 30/60 [09:12<09:09, 18.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9119363421539091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|    | 31/60 [09:31<08:51, 18.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9123066326357284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|    | 32/60 [09:49<08:33, 18.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9107782868844159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|    | 33/60 [10:07<08:15, 18.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.909916748415749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|    | 34/60 [10:26<07:55, 18.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.909356249390908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|    | 35/60 [10:44<07:36, 18.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9097467188565236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|    | 36/60 [11:02<07:18, 18.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.908678301662769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|   | 37/60 [11:20<06:59, 18.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9072371788744656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|   | 38/60 [11:38<06:40, 18.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9083543380476394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|   | 39/60 [11:57<06:22, 18.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9065745568500375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|   | 40/60 [12:15<06:03, 18.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9067405878372912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|   | 41/60 [12:33<05:45, 18.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9064526884060986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|   | 42/60 [12:51<05:26, 18.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9064558075284058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|  | 43/60 [13:09<05:08, 18.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9046175648581307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|  | 44/60 [13:27<04:50, 18.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9041656688699182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|  | 45/60 [13:45<04:32, 18.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9047579754073665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|  | 46/60 [14:03<04:13, 18.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9043326580299521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|  | 47/60 [14:22<03:55, 18.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9027422863357472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|  | 48/60 [14:40<03:37, 18.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9024317337656921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%| | 49/60 [14:58<03:19, 18.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9020068195630919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%| | 50/60 [15:16<03:01, 18.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9018031083187967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%| | 51/60 [15:34<02:43, 18.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.902336337094037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%| | 52/60 [15:52<02:25, 18.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9004793813768422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%| | 53/60 [16:10<02:07, 18.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9019491256407972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%| | 54/60 [16:29<01:48, 18.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9002289513372025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|| 55/60 [16:47<01:30, 18.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9001046632820705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|| 56/60 [17:05<01:12, 18.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8997023201213693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|| 57/60 [17:23<00:54, 18.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8990964450926151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|| 58/60 [17:41<00:36, 18.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8983574335305196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|| 59/60 [17:59<00:18, 18.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8976209607889067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [18:17<00:00, 18.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8982535235162051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(train_paths)} images for training and {len(test_paths)} images for testing.')\n",
    "\n",
    "vocab = get_vocab(train_descriptions, word_count_threshold=10)\n",
    "idxtoword, wordtoidx = get_word_dict(vocab)\n",
    "vocab_size = get_vocab_size(idxtoword)\n",
    "embedding_dim = 200\n",
    "embedding_matrix = get_embeddings(embeddings_index, vocab_size, embedding_dim, wordtoidx) \n",
    "\n",
    "print(f'Preparing dataloader...')\n",
    "train_img_features, test_img_features, sydney_img_features = get_train_test(encoder, train_paths, test_paths, sydney_paths)\n",
    "\n",
    "train_loader = get_train_dataloader(\n",
    "    train_descriptions, \n",
    "    train_img_features,\n",
    "    wordtoidx,\n",
    "    max_length,\n",
    "    batch_size=200\n",
    ")\n",
    "\n",
    "print(f'Training...')\n",
    "caption_model = train_model(\n",
    "    train_loader,\n",
    "    vocab_size,\n",
    "    embedding_dim, \n",
    "    embedding_matrix\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_score = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating captions...\n",
      "tokenization...\n",
      "computing Bleu score...\n",
      "computing METEOR score...\n",
      "computing Rouge score...\n",
      "computing CIDEr score...\n",
      "computing SPICE score...\n",
      "computing Universal_Sentence_Encoder_Similarity score...\n"
     ]
    }
   ],
   "source": [
    "model_score['test'] = evaluate_results(\n",
    "    test_img_features, \n",
    "    caption_model,\n",
    "    test_descriptions,\n",
    "    max_length,\n",
    "    vocab_size,\n",
    "    wordtoidx,\n",
    "    idxtoword\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating captions...\n",
      "tokenization...\n",
      "computing Bleu score...\n",
      "computing METEOR score...\n",
      "computing Rouge score...\n",
      "computing CIDEr score...\n",
      "computing SPICE score...\n",
      "computing Universal_Sentence_Encoder_Similarity score...\n"
     ]
    }
   ],
   "source": [
    "model_score['sydney'] = evaluate_results(\n",
    "    sydney_img_features, \n",
    "    caption_model,\n",
    "    sydney_descriptions,\n",
    "    max_length,\n",
    "    vocab_size,\n",
    "    wordtoidx,\n",
    "    idxtoword\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': {'Bleu_1': 0.6136827724272327,\n",
       "  'Bleu_2': 0.4855041477977092,\n",
       "  'Bleu_3': 0.402549797666307,\n",
       "  'Bleu_4': 0.34299301131647947,\n",
       "  'METEOR': 0.2782934215070498,\n",
       "  'ROUGE_L': 0.5195925891444665,\n",
       "  'CIDEr': 1.8834414468462415,\n",
       "  'SPICE': 0.36436093016411836,\n",
       "  'USC_similarity': 0.5813592605732538},\n",
       " 'sydney': {'Bleu_1': 0.4714297101589688,\n",
       "  'Bleu_2': 0.23656988360614592,\n",
       "  'Bleu_3': 0.12426535818465753,\n",
       "  'Bleu_4': 0.0744954207775621,\n",
       "  'METEOR': 0.15287495794291223,\n",
       "  'ROUGE_L': 0.30265197078482137,\n",
       "  'CIDEr': 0.20496019594778098,\n",
       "  'SPICE': 0.12656755505525322,\n",
       "  'USC_similarity': 0.47996828951439896}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = '9.1.1.2'\n",
    "with open(f'{root_captioning}/fz_notebooks/final_results_n{tag}.json', 'w') as fp:\n",
    "    json.dump(model_score, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "9-fz-baseline_model_pytorch_v3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}