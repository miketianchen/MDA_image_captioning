{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lYUHk6sxO2pO"
   },
   "source": [
    "# CNN trained on cactus data\n",
    "\n",
    "## Adapted from DSCI 572 Lab 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AoVG1Z3FO2pQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "from skimage.io import imread\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from time import time\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import chain\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G-ePax-DO2pS"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting image paths and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab_type": "text",
    "id": "nvsR_W_tO2qN"
   },
   "outputs": [],
   "source": [
    "root_path = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_paths():\n",
    "    \"\"\"\n",
    "    Returns img paths for cactus dataset\n",
    "\n",
    "    --------\n",
    "    list, list, list, list: \n",
    "        training image paths, testing image paths, training classes, testing classes\n",
    "    \"\"\"\n",
    "    train_img_path = []\n",
    "    train_img_class = []\n",
    "    test_img_path = []\n",
    "    test_img_class = []\n",
    "    for root, dirs, files in os.walk(os.path.abspath(root_path + \"training_set/training_set/no_cactus\")):\n",
    "        for file in files:\n",
    "            train_img_path.append(file)\n",
    "            train_img_class.append(\"no_cactus\")\n",
    "    for root, dirs, files in os.walk(os.path.abspath(root_path + \"training_set/training_set/cactus\")):\n",
    "        for file in files:\n",
    "            train_img_path.append(file)\n",
    "            train_img_class.append(\"cactus\")\n",
    "    for root, dirs, files in os.walk(os.path.abspath(root_path + \"validation_set/validation_set/no_cactus\")):\n",
    "        for file in files:\n",
    "            test_img_path.append(file)\n",
    "            test_img_class.append(\"no_cactus\")\n",
    "    for root, dirs, files in os.walk(os.path.abspath(root_path + \"validation_set/validation_set/cactus\")):\n",
    "        for file in files:\n",
    "            test_img_path.append(file)\n",
    "            test_img_class.append(\"cactus\")\n",
    "    return train_img_path, test_img_path, train_img_class, test_img_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17500"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_img_paths()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_img_paths()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path, test_path, train_class, test_class = get_img_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sinplanta.3141.jpg'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no_cactus'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_class[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17500\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(train_class))\n",
    "print(len(set(train_class)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cactus', 'no_cactus'}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = set(train_class)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cactus': 0, 'no_cactus': 1}\n"
     ]
    }
   ],
   "source": [
    "d = dict()\n",
    "i = 0\n",
    "for label in labels:\n",
    "    if label not in d:\n",
    "        d[label] = i\n",
    "        i += 1\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training baseline model with 1 color channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 32, 32)\n",
      "(3000,)\n"
     ]
    }
   ],
   "source": [
    "train_img = []\n",
    "y_train = []\n",
    "train_length = 3000 #len(train_path)\n",
    "for i in range(0, train_length):\n",
    "    # defining the image path\n",
    "    if(train_class[i] == \"cactus\"):\n",
    "        image_path = root_path + \"training_set/training_set/cactus/\" + train_path[i]\n",
    "    else:\n",
    "        image_path = root_path + \"training_set/training_set/no_cactus/\" + train_path[i]\n",
    "    # reading the image\n",
    "    img = imread(root_path + image_path, as_gray=True)\n",
    "    # normalizing the pixel values\n",
    "    img /= 255.0\n",
    "    # converting the type of pixel to float 32\n",
    "    img = img.astype('float32')\n",
    "    # appending the image into the list\n",
    "    train_img.append(img)\n",
    "    y_train.append(train_class[i])\n",
    "    \n",
    "# converting the list to numpy array\n",
    "X_train = np.array(train_img)\n",
    "# defining the target\n",
    "X_train.shape\n",
    "y_train = np.array(y_train)\n",
    "for i in range(0, len(y_train)):\n",
    "    y_train[i] = d[y_train[i]]\n",
    "y_train = y_train.astype(int);\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 32, 32)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "test_img = []\n",
    "y_test = []\n",
    "test_length = 1000 #len(test_path)\n",
    "for i in range(0, test_length):\n",
    "    # defining the image path\n",
    "    if(test_class[i] == \"cactus\"):\n",
    "        image_path = root_path + \"validation_set/validation_set/cactus/\" + test_path[i]\n",
    "    else:\n",
    "        image_path = root_path + \"validation_set/validation_set/no_cactus/\" + test_path[i]\n",
    "    # reading the image\n",
    "    img = imread(root_path + image_path, as_gray=True)\n",
    "    # normalizing the pixel values\n",
    "    img /= 255.0\n",
    "    # converting the type of pixel to float 32\n",
    "    img = img.astype('float32')\n",
    "    # appending the image into the list\n",
    "    test_img.append(img)\n",
    "    y_test.append(test_class[i])\n",
    "    \n",
    "# converting the list to numpy array\n",
    "X_test = np.array(test_img)\n",
    "# defining the target\n",
    "X_test.shape\n",
    "y_test = np.array(y_test)\n",
    "for i in range(0, len(y_test)):\n",
    "    y_test[i] = d[y_test[i]]\n",
    "y_test = y_test.astype(int);\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vc8-vt_WO2qQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 32, 32, 1)\n",
      "(3000, 2)\n",
      "(1000, 32, 32, 1)\n",
      "(1000, 2)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# reshape to be [samples][channels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 32, 32, 1).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 32, 32, 1).astype('float32')\n",
    "\n",
    "# one hot encode outputs\n",
    "y_train = utils.to_categorical(y_train)\n",
    "y_test = utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = list(zip(X_train, y_train))\n",
    "random.shuffle(combined)\n",
    "\n",
    "X_train[:], y_train[:] = zip(*combined)\n",
    "\n",
    "combined = list(zip(X_test, y_test))\n",
    "random.shuffle(combined)\n",
    "\n",
    "X_test[:], y_test[:] = zip(*combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa3e506d910>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0].reshape([32,32]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LodO4nxkO2qV"
   },
   "outputs": [],
   "source": [
    "# define a simple CNN model\n",
    "def build_mnist_CNN():\n",
    "    mnist_model = Sequential()\n",
    "    mnist_model.add(Conv2D(32, (5, 5), input_shape=(32, 32, 1), activation='relu'))\n",
    "    mnist_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    mnist_model.add(Dropout(0.2))\n",
    "    mnist_model.add(Flatten())\n",
    "    mnist_model.add(Dense(128, activation='relu'))\n",
    "    mnist_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Compile model\n",
    "    mnist_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return mnist_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_model = build_mnist_CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MXvKbAJ7O2qW",
    "outputId": "e197f9bf-d87d-44e8-c54a-e55c4dfc0b7f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 0.3968 - accuracy: 0.9997 - val_loss: 0.0248 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "12/12 [==============================] - 1s 78ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 3.0953e-05 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 9.0255e-06 - accuracy: 1.0000 - val_loss: 1.2664e-06 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "12/12 [==============================] - 1s 77ms/step - loss: 7.2662e-07 - accuracy: 1.0000 - val_loss: 3.9470e-07 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "12/12 [==============================] - 1s 77ms/step - loss: 3.2504e-07 - accuracy: 1.0000 - val_loss: 2.6321e-07 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "12/12 [==============================] - 1s 80ms/step - loss: 2.5761e-07 - accuracy: 1.0000 - val_loss: 2.4664e-07 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "12/12 [==============================] - 1s 82ms/step - loss: 2.3921e-07 - accuracy: 1.0000 - val_loss: 2.4247e-07 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 2.3580e-07 - accuracy: 1.0000 - val_loss: 2.4045e-07 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 2.3290e-07 - accuracy: 1.0000 - val_loss: 2.3937e-07 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "12/12 [==============================] - 1s 82ms/step - loss: 2.3023e-07 - accuracy: 1.0000 - val_loss: 2.3854e-07 - val_accuracy: 1.0000\n",
      "CNN Error: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "mnist_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=256)\n",
    "# Final evaluation of the model\n",
    "scores = mnist_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This dataset with 2 classes is simple enough that a shallow CNN can achieve 100% validation accuracy almost immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "lab4.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
