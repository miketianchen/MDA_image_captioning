{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_captioning = \"../../data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ra-d7BLlf1nC"
   },
   "outputs": [],
   "source": [
    "def get_img_info(name, num=np.inf):\n",
    "    \"\"\"\n",
    "    Returns img paths and captions\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    name: str\n",
    "        the json file name\n",
    "    num: int (default: np.inf)\n",
    "        the number of observations to get\n",
    "\n",
    "    Return:\n",
    "    --------\n",
    "    list, dict, int\n",
    "        img paths, corresponding captions, max length of captions\n",
    "    \"\"\"\n",
    "    img_path = []\n",
    "    caption = [] \n",
    "    max_length = 0\n",
    "    if AWS:\n",
    "        with open(f'{root_captioning}/json/{name}.json', 'r') as json_data:\n",
    "            data = json.load(json_data)\n",
    "            for filename in data.keys():\n",
    "                if num is not None and len(caption) == num:\n",
    "                    break\n",
    "                img_path.append(\n",
    "                    f'{root_captioning}/{name}/{filename}'\n",
    "                )\n",
    "                sen_list = []\n",
    "                for sentence in data[filename]['sentences']:\n",
    "                    max_length = max(max_length, len(sentence['tokens']))\n",
    "                    sen_list.append(sentence['raw'])\n",
    "\n",
    "                caption.append(sen_list)    \n",
    "    else:            \n",
    "        with open(f'{root_captioning}/interim/{name}.json', 'r') as json_data:\n",
    "            data = json.load(json_data)\n",
    "            for set_name in ['rsicd', 'ucm']:\n",
    "                for filename in data[set_name].keys():\n",
    "                    if num is not None and len(caption) == num:\n",
    "                        break\n",
    "\n",
    "                    img_path.append(\n",
    "                        f'{root_captioning}/raw/imgs/{set_name}/{filename}'\n",
    "                    )\n",
    "                    sen_list = []\n",
    "                    for sentence in data[set_name][filename]['sentences']:\n",
    "                        max_length = max(max_length, len(sentence['tokens']))\n",
    "                        sen_list.append(sentence['raw'])\n",
    "\n",
    "                    caption.append(sen_list)\n",
    "    \n",
    "    return img_path, caption, max_length            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zMUxfR9xKICI"
   },
   "outputs": [],
   "source": [
    "# get img path and caption list\n",
    "# only test 800 train samples and 200 valid samples\n",
    "# train_paths, train_descriptions, max_length_train = get_img_info('train', 800)\n",
    "# test_paths, test_descriptions, max_length_test = get_img_info('valid', 200)\n",
    "\n",
    "train_paths, train_descriptions, max_length_train = get_img_info('train')\n",
    "test_paths, test_descriptions, max_length_test = get_img_info('valid')\n",
    "max_length = max(max_length_train, max_length_test)\n",
    "\n",
    "      \n",
    "lex = set()\n",
    "for sen in train_descriptions:\n",
    "    [lex.update(d.split()) for d in sen]\n",
    "\n",
    "for sen in test_descriptions:\n",
    "    [lex.update(d.split()) for d in sen]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xdFUtyIPgaOy"
   },
   "source": [
    "Stats on what was collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "Gnq3ZjaSU79-",
    "outputId": "b7a53a78-c75e-4fbd-ab86-336c9b6c65d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8332\n",
      "2084\n",
      "2912\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "print(len(train_descriptions)) # How many images? \n",
    "print(len(test_descriptions)) # How many images? \n",
    "print(len(lex)) # How many unique words (vocab)\n",
    "print(max_length) # Maximum length of a caption (in words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aP2-k6XfgaPT"
   },
   "source": [
    "Display the size of the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ZZQZkT8X_qNt",
    "outputId": "17c78e0b-2241-4465-8716-9b255b2a917a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8332\n",
      "2084\n"
     ]
    }
   ],
   "source": [
    "print(len(train_paths))\n",
    "print(len(test_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2ZQ_Cl71YM55",
    "outputId": "69bb5df4-20fc-4297-f83c-9b3fcb6b1363"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../s3/train/ucm_1080.jpg'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_paths[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iawCdtU4gaPa"
   },
   "source": [
    "Build the sequences.  We include a **start** and **stop** token at the beginning/end.  We will later use the **start** token to begin the process of generating a caption.  Encountering the **stop** token in the generated text will let us know we are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cBX6TJbPF0nD"
   },
   "outputs": [],
   "source": [
    "for v in train_descriptions: \n",
    "    for d in range(len(v)):\n",
    "        v[d] = f'{START} {v[d]} {STOP}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XHD6IZ3HgaPk"
   },
   "source": [
    "See how many discriptions were extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "colab_type": "code",
    "id": "v3_PCjddHcUe",
    "outputId": "6f86ed9d-5da3-4d9a-ef3d-151a5cbb1f6e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['startseq Lots of boats docked at the harbor and the boats are closed to each other . endseq',\n",
       " 'startseq Lots of boats docked neatly at the harbor . endseq',\n",
       " 'startseq Many boats docked neatly at the harbor and the water is deep blue . endseq',\n",
       " 'startseq Many boats docked neatly at the harbor and some positions are free . endseq',\n",
       " 'startseq Lots of boats docked neatly at the harbor and the boats are closed to each other . endseq']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_descriptions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "C8DFzzPmXg-n",
    "outputId": "b84b25c0-fcbd-4219-aafc-eaf555fdd7c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52080"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_captions = []\n",
    "for descriptions in [train_descriptions, test_descriptions]:\n",
    "    for val in descriptions:\n",
    "        for cap in val:\n",
    "            all_captions.append(cap)\n",
    "len(all_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "w2vQHy7Ja2Py",
    "outputId": "e7c59fab-5755-4f60-f1cd-74a2322f3eb2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a vast artificial lake was built in the park .'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_captions[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "w2vQHy7Ja2Py",
    "outputId": "e7c59fab-5755-4f60-f1cd-74a2322f3eb2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'startseq Lots of boats docked at the harbor and the boats are closed to each other . endseq'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_captions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-HxHSgLgYUUE",
    "outputId": "ea31af55-684d-4e7b-ca23-01df2412ffd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2915 words.\n"
     ]
    }
   ],
   "source": [
    "all_word_counts = {}\n",
    "nsents = 0\n",
    "for sent in all_captions:\n",
    "    nsents += 1\n",
    "    for w in sent.split(' '):\n",
    "        all_word_counts[w] = all_word_counts.get(w, 0) + 1\n",
    "\n",
    "all_vocab = [w for w in all_word_counts]\n",
    "print(f'Found {len(all_vocab)} words.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vtIIFt4m6pTv"
   },
   "source": [
    "### Loading Wikipedia2vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\n",
      "4520033it [10:10, 7414.19it/s]\u001b[A\n",
      "4520787it [10:10, 7448.57it/s]\u001b[A\n",
      "4521542it [10:10, 7478.09it/s]\u001b[A\n",
      "4522299it [10:10, 7504.18it/s]\u001b[A\n",
      "4523050it [10:10, 7462.57it/s]\u001b[A\n",
      "4523798it [10:10, 7466.95it/s]\u001b[A\n",
      "4524545it [10:11, 7432.35it/s]\u001b[A\n",
      "4525289it [10:11, 7385.06it/s]\u001b[A\n",
      "4526028it [10:11, 7264.30it/s]\u001b[A\n",
      "4526755it [10:11, 7174.87it/s]\u001b[A\n",
      "4527485it [10:11, 7209.47it/s]\u001b[A\n",
      "4528207it [10:11, 7206.07it/s]\u001b[A\n",
      "4528946it [10:11, 7258.89it/s]\u001b[A\n",
      "4530030it [10:11, 7404.62it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4530030 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {} \n",
    "\n",
    "f = open(\n",
    "    f'{root_captioning}/download/enwiki_20180420_500d.txt', \n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "f.readline()\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = ' '.join(values[:-500])\n",
    "    coefs = np.asarray(values[-500:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "\n",
    "f.close()\n",
    "print(f'Found {len(embeddings_index)} word vectors.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2338 words out of 2915 are fount in the pre-trained matrix.\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 500\n",
    "\n",
    "# Get 500-dim dense vector for each of the 4530030 words in out vocabulary\n",
    "embedding_matrix = {}\n",
    "count = 0\n",
    "for word in all_word_counts.keys():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        count += 1\n",
    "        # Words not found in the embedding index will be all zeros\n",
    "        embedding_matrix[word] = embedding_vector.tolist()\n",
    "        \n",
    "print(f'{count} out of {len(all_vocab)} words are found in the pre-trained matrix.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the embedding matrix \n",
    "with open(f'{root_captioning}/enwiki_20180420_2338_words_500d.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(embedding_matrix, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the embedding matrix \n",
    "with open(f'{root_captioning}/enwiki_20180420_2338_words_500d.json', 'r', encoding='utf-8') as file:\n",
    "    em = json.load(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}