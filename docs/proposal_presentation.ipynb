{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Image Captioning of Earth Observational Imagery\n",
    "\n",
    "## An MDS-MDA Joint Capstone Project\n",
    "\n",
    "\n",
    "<img src=\"../imgs/mda.png\" alt=\"drawing\" style=\"width:600px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### Dora Qian, Fanli Zhou, James Huang, Mike Chen\n",
    "\n",
    "<font size=\"2\">[MDA logo](https://mdacorporation.com/corporate/)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MDA\n",
    "#### A Canadian Aerospace Company\n",
    "\n",
    "+ Developed Canadarm and Canadarm-2 on the ISS\n",
    "\n",
    "<img src=\"../imgs/canadarm.jpg\" alt=\"drawing\" style=\"width:800px;\" class=\"center\"/>\n",
    "\n",
    "<font size=\"2\">Sources: \n",
    "[Canadarm](https://www.thecanadianencyclopedia.ca/en/article/canadarm), \n",
    "[Canadarm2](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e5/STS-114_Steve_Robinson_on_Canadarm2.jpg/2560px-STS-114_Steve_Robinson_on_Canadarm2.jpg)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MDA\n",
    "\n",
    "+ Access to a vast database of satellite images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"../imgs/505.png\" alt=\"drawing\" style=\"width:400px;\"/>\n",
    "\n",
    "<font size=\"2\">Sources: Image adapted from Qu, B. et al. (2016) [1].</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The problem\n",
    "\n",
    "+ These images are uncaptioned\n",
    "    + Without captions, these images are difficult and computationally costly to work with\n",
    "+ Technology of captioning satellite images is less mature than \"traditional\" photographs\n",
    "+ Due the nature of these photographs, the model cannot be effectively trained on other types of images.\n",
    "    + Limited existing resources to train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Image Captioning: Motivation and Purpose\n",
    "\n",
    "+ Associating an image with a caption makes it much more accessible:\n",
    "\n",
    "    + Tag and sort images based on content\n",
    "\n",
    "    + Return search queries\n",
    "    \n",
    "    + Evaluate image similarity\n",
    "    \n",
    "    + Downstream applications\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training the Model\n",
    "\n",
    "+ MDA images are uncaptioned\n",
    " + Train, validate, and test on public, captioned satellite images\n",
    "     + Several different datasets\n",
    "     + Assess cross dataset performance\n",
    " + Final manual evaluation on uncaptioned MDA images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Final Data Product\n",
    "- End-to-End image captioning pipeline \n",
    "- 3 independent modules\n",
    "<img src=\"../imgs/dataproduct.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Final Data Product: Database\n",
    "- Non-relational database\n",
    "- Stores both human-annotated and machine-generated image-caption pairs\n",
    "<img src=\"../imgs/database.png\" width=\"600\">\n",
    "\n",
    "\n",
    "<font size=\"2\">Sources: [RSICD_optimal](https://github.com/201528014227051/RSICD_optimal)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Final Data Product: Deep Learning Model\n",
    "- Load images from database\n",
    "- Model training\n",
    "- Model prediction\n",
    "- Easy to update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"../imgs/aws.png\" width=\"300\">\n",
    "\n",
    "<img src=\"../imgs/Pytorch_logo.png\" width=\"400\">\n",
    "\n",
    "<font size=\"2\">Sources: \n",
    "[AWS logo](https://commons.wikimedia.org/wiki/File:Amazon_Web_Services_Logo.svg), \n",
    "[Pytorch logo](https://commons.wikimedia.org/wiki/File:Pytorch_logo.png)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Final Data Product: Visualization & Database Updating Tool\n",
    "\n",
    "- Random selected images from database\n",
    "- Self upload images outside database\n",
    "- Standardize the image, make prediction and save to database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<img src=\"../imgs/tool.png\" width=\"800\">\n",
    "\n",
    "<font size=\"2\">Sources: [RSICD_optimal](https://github.com/201528014227051/RSICD_optimal)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Description\n",
    "\n",
    "There are three labeled datasets:\n",
    "- UCM_Captions \n",
    "- Sydney_Captions\n",
    "- RSICD (Remote Sensing Imaging Captioning Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### UCM_captions\n",
    "\n",
    "- 21 Different Classes of Images\n",
    "- 2100 Different Images \n",
    "- 256 X 256 Pixels\n",
    "- .tif Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "\n",
    "<table><tr><td><img src='../imgs/ucm_1.jpg' width=\"200\" height=\"80\"></td><td><img src='../imgs/ucm_2.jpg' width=\"200\" height=\"80\"></td></tr></table>\n",
    "\n",
    "<table><tr><td><img src='../imgs/ucm_3.jpg' width=\"200\" height=\"80\"></td><td><img src='../imgs/ucm_4.jpg' width=\"200\" height=\"80\"></td></tr></table>\n",
    "\n",
    "<font size=\"2\">Sources: [UC Merced Land Use Dataset](http://weegee.vision.ucmerced.edu/datasets/landuse.html)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sydney_captions\n",
    "\n",
    "- 7 Different Classes of Images\n",
    "- 613 Different Images \n",
    "- 500 X 500 Pixels \n",
    "- .tif Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<table><tr><td><img src='../imgs/sydney_1.jpg' width=\"200\" height=\"30\"></td><td><img src='../imgs/sydney_2.jpg' width=\"200\" height=\"30\"></td></tr></table>\n",
    "\n",
    "\n",
    "<table><tr><td><img src='../imgs/sydney_3.jpg' width=\"200\" height=\"30\"></td><td><img src='../imgs/syndey_4.jpg' width=\"200\" height=\"30\"></td></tr></table>\n",
    "\n",
    "<font size=\"2\">Sources: Image adapted from Qu, B. et al. (2016) [1].</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RSICD (Remote Sensing Imaging Captioning Dataset)\n",
    "\n",
    "\n",
    "\n",
    "- 10,922 Different Images \n",
    "- 224 X 224 Pixels\n",
    "- .jpg Format \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<table><tr><td><img src='../imgs/rsicd_1.jpg' width=\"200\" height=\"80\"></td><td><img src='../imgs/rsicd_2.jpg' width=\"200\" height=\"80\"></td></tr></table>\n",
    "\n",
    "<table><tr><td><img src='../imgs/rsicd_3.jpg' width=\"200\" height=\"80\"></td><td><img src='../imgs/rsicd_4.jpg' width=\"200\" height=\"80\"></td></tr></table>\n",
    "\n",
    "<font size=\"2\">Sources: [RSICD_optimal](https://github.com/201528014227051/RSICD_optimal)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RSICD Caption Example\n",
    "<img src=\"../imgs/rsicd_caption.png\" width=\"800\">\n",
    "\n",
    "\n",
    "<font size=\"2\">Image adapted from Lu, X. et al. (2018) [1].</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exploratory Data Analysis \n",
    "- Train_Valid/Test = 80%/20%\n",
    "- Train/Validation = 80%/20%\n",
    "- Maximum Length: 34 words\n",
    "- Minimum Length: 2 words\n",
    "- Most Common Words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"../imgs/word_cloud1.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Science Techniques\n",
    "### Baseline Model: CNN + RNN (LSTM)\n",
    "\n",
    "<img src=\"../imgs/model_1.png\" width=\"1200\">\n",
    "\n",
    "<font size=\"2\">Sources: Image adapted from Lu, X. et al. (2018) [2].</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model II: CNN + Attention + LSTM\n",
    "\n",
    "<img src=\"../imgs/model_2.png\" width=\"1200\">\n",
    "\n",
    "<font size=\"2\">Sources: Image adapted from Zhang, X. et al. (2019) [3].</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model III: CNN + multi-level Attention + LSTM\n",
    "\n",
    "<img src=\"../imgs/model_3.png\" width=\"1000\">\n",
    "\n",
    "<font size=\"2\">Sources: Image adapted from Li, Y. et al. (2020) [4].</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Timeline and Evaluation\n",
    "<img src=\"../imgs/timeline.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reference\n",
    "\n",
    "<font size=\"4\">1. B. Qu, X. Li, D. Tao, and X. Lu, “Deep semantic understanding of high resolution remote sensing image,” International Conference on Computer, Information and Telecommunication Systems, pp. 124–128, 2016.</font>\n",
    "\n",
    "<font size=\"4\">2. Lu, X.; Wang, B.; Zheng, X.; Li, X. Exploring models and data for remote sensing image caption generation. IEEE Trans. Geosci. Remote Sens. 2018, 56, 2183–2195.</font>\n",
    "\n",
    "<font size=\"4\">3. Zhang, X.; Wang, X.; Tang, X.; Zhou, H.; Li, C. Description Generation for Remote Sensing Images Using Attribute Attention Mechanism. Remote Sens. 2019, 11, 612.</font>\n",
    "\n",
    "<font size=\"4\">4. Li, Y.; Fang, S.; Jiao, L.; Liu, R.; Shang, R. A Multi-Level Attention Model for Remote Sensing Image Captions. Remote Sens. 2020, 12, 939.</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../imgs/thankyou.png\" width=\"900\">"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "rise": {
   "autolaunch": false,
   "backimage": "../imgs/template.png",
   "start_slideshow_at": "selected",
   "theme": "serif",
   "transition": "fade"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
